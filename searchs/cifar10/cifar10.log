06/28 09:22:15 PM | 
06/28 09:22:15 PM | Parameters:
06/28 09:22:15 PM | ALPHA_LR=0.0003
06/28 09:22:15 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:22:15 PM | BATCH_SIZE=64
06/28 09:22:15 PM | DATA_PATH=./data/
06/28 09:22:15 PM | DATASET=cifar10
06/28 09:22:15 PM | EPOCHS=50
06/28 09:22:15 PM | GPUS=[0]
06/28 09:22:15 PM | INIT_CHANNELS=16
06/28 09:22:15 PM | LAYERS=8
06/28 09:22:15 PM | NAME=cifar10
06/28 09:22:15 PM | PATH=searchs/cifar10
06/28 09:22:15 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:22:15 PM | PRINT_FREQ=50
06/28 09:22:15 PM | SEED=2
06/28 09:22:15 PM | W_GRAD_CLIP=5.0
06/28 09:22:15 PM | W_LR=0.025
06/28 09:22:15 PM | W_LR_MIN=0.001
06/28 09:22:15 PM | W_MOMENTUM=0.9
06/28 09:22:15 PM | W_WEIGHT_DECAY=0.0003
06/28 09:22:15 PM | WORKERS=4
06/28 09:22:15 PM | 
06/28 09:22:15 PM | Logger is set - training start
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Parameters:
06/28 09:28:51 PM | ALPHA_LR=0.0003
06/28 09:28:51 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:28:51 PM | BATCH_SIZE=64
06/28 09:28:51 PM | DATA_PATH=./data/
06/28 09:28:51 PM | DATASET=cifar10
06/28 09:28:51 PM | EPOCHS=50
06/28 09:28:51 PM | GPUS=[0]
06/28 09:28:51 PM | INIT_CHANNELS=16
06/28 09:28:51 PM | LAYERS=8
06/28 09:28:51 PM | NAME=cifar10
06/28 09:28:51 PM | PATH=searchs/cifar10
06/28 09:28:51 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:28:51 PM | PRINT_FREQ=50
06/28 09:28:51 PM | SEED=2
06/28 09:28:51 PM | W_GRAD_CLIP=5.0
06/28 09:28:51 PM | W_LR=0.025
06/28 09:28:51 PM | W_LR_MIN=0.001
06/28 09:28:51 PM | W_MOMENTUM=0.9
06/28 09:28:51 PM | W_WEIGHT_DECAY=0.0003
06/28 09:28:51 PM | WORKERS=4
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Logger is set - training start
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Parameters:
06/28 09:30:37 PM | ALPHA_LR=0.0003
06/28 09:30:37 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:30:37 PM | BATCH_SIZE=64
06/28 09:30:37 PM | DATA_PATH=./data/
06/28 09:30:37 PM | DATASET=cifar10
06/28 09:30:37 PM | EPOCHS=50
06/28 09:30:37 PM | GPUS=[0]
06/28 09:30:37 PM | INIT_CHANNELS=16
06/28 09:30:37 PM | LAYERS=8
06/28 09:30:37 PM | NAME=cifar10
06/28 09:30:37 PM | PATH=searchs/cifar10
06/28 09:30:37 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:30:37 PM | PRINT_FREQ=50
06/28 09:30:37 PM | SEED=2
06/28 09:30:37 PM | W_GRAD_CLIP=5.0
06/28 09:30:37 PM | W_LR=0.025
06/28 09:30:37 PM | W_LR_MIN=0.001
06/28 09:30:37 PM | W_MOMENTUM=0.9
06/28 09:30:37 PM | W_WEIGHT_DECAY=0.0003
06/28 09:30:37 PM | WORKERS=4
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Parameters:
06/28 09:31:11 PM | ALPHA_LR=0.0003
06/28 09:31:11 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:11 PM | BATCH_SIZE=64
06/28 09:31:11 PM | DATA_PATH=./data/
06/28 09:31:11 PM | DATASET=cifar10
06/28 09:31:11 PM | EPOCHS=50
06/28 09:31:11 PM | GPUS=[0]
06/28 09:31:11 PM | INIT_CHANNELS=16
06/28 09:31:11 PM | LAYERS=8
06/28 09:31:11 PM | NAME=cifar10
06/28 09:31:11 PM | PATH=searchs/cifar10
06/28 09:31:11 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:11 PM | PRINT_FREQ=50
06/28 09:31:11 PM | SEED=2
06/28 09:31:11 PM | W_GRAD_CLIP=5.0
06/28 09:31:11 PM | W_LR=0.025
06/28 09:31:11 PM | W_LR_MIN=0.001
06/28 09:31:11 PM | W_MOMENTUM=0.9
06/28 09:31:11 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:11 PM | WORKERS=4
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Parameters:
06/28 09:31:44 PM | ALPHA_LR=0.0003
06/28 09:31:44 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:44 PM | BATCH_SIZE=64
06/28 09:31:44 PM | DATA_PATH=./data/
06/28 09:31:44 PM | DATASET=cifar10
06/28 09:31:44 PM | EPOCHS=50
06/28 09:31:44 PM | GPUS=[0]
06/28 09:31:44 PM | INIT_CHANNELS=16
06/28 09:31:44 PM | LAYERS=8
06/28 09:31:44 PM | NAME=cifar10
06/28 09:31:44 PM | PATH=searchs/cifar10
06/28 09:31:44 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:44 PM | PRINT_FREQ=50
06/28 09:31:44 PM | SEED=2
06/28 09:31:44 PM | W_GRAD_CLIP=5.0
06/28 09:31:44 PM | W_LR=0.025
06/28 09:31:44 PM | W_LR_MIN=0.001
06/28 09:31:44 PM | W_MOMENTUM=0.9
06/28 09:31:44 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:44 PM | WORKERS=4
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Parameters:
06/28 09:37:36 PM | ALPHA_LR=0.0003
06/28 09:37:36 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:37:36 PM | BATCH_SIZE=64
06/28 09:37:36 PM | DATA_PATH=./data/
06/28 09:37:36 PM | DATASET=cifar10
06/28 09:37:36 PM | EPOCHS=50
06/28 09:37:36 PM | GPUS=[0]
06/28 09:37:36 PM | INIT_CHANNELS=16
06/28 09:37:36 PM | LAYERS=8
06/28 09:37:36 PM | NAME=cifar10
06/28 09:37:36 PM | PATH=searchs/cifar10
06/28 09:37:36 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:37:36 PM | PRINT_FREQ=50
06/28 09:37:36 PM | SEED=2
06/28 09:37:36 PM | W_GRAD_CLIP=5.0
06/28 09:37:36 PM | W_LR=0.025
06/28 09:37:36 PM | W_LR_MIN=0.001
06/28 09:37:36 PM | W_MOMENTUM=0.9
06/28 09:37:36 PM | W_WEIGHT_DECAY=0.0003
06/28 09:37:36 PM | WORKERS=4
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Parameters:
06/28 09:38:10 PM | ALPHA_LR=0.0003
06/28 09:38:10 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:38:10 PM | BATCH_SIZE=64
06/28 09:38:10 PM | DATA_PATH=./data/
06/28 09:38:10 PM | DATASET=cifar10
06/28 09:38:10 PM | EPOCHS=50
06/28 09:38:10 PM | GPUS=[0]
06/28 09:38:10 PM | INIT_CHANNELS=16
06/28 09:38:10 PM | LAYERS=8
06/28 09:38:10 PM | NAME=cifar10
06/28 09:38:10 PM | PATH=searchs/cifar10
06/28 09:38:10 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:38:10 PM | PRINT_FREQ=50
06/28 09:38:10 PM | SEED=2
06/28 09:38:10 PM | W_GRAD_CLIP=5.0
06/28 09:38:10 PM | W_LR=0.025
06/28 09:38:10 PM | W_LR_MIN=0.001
06/28 09:38:10 PM | W_MOMENTUM=0.9
06/28 09:38:10 PM | W_WEIGHT_DECAY=0.0003
06/28 09:38:10 PM | WORKERS=4
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Parameters:
06/28 09:40:03 PM | ALPHA_LR=0.0003
06/28 09:40:03 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:40:03 PM | BATCH_SIZE=64
06/28 09:40:03 PM | DATA_PATH=./data/
06/28 09:40:03 PM | DATASET=cifar10
06/28 09:40:03 PM | EPOCHS=50
06/28 09:40:03 PM | GPUS=[0]
06/28 09:40:03 PM | INIT_CHANNELS=16
06/28 09:40:03 PM | LAYERS=8
06/28 09:40:03 PM | NAME=cifar10
06/28 09:40:03 PM | PATH=searchs/cifar10
06/28 09:40:03 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:40:03 PM | PRINT_FREQ=50
06/28 09:40:03 PM | SEED=2
06/28 09:40:03 PM | W_GRAD_CLIP=5.0
06/28 09:40:03 PM | W_LR=0.025
06/28 09:40:03 PM | W_LR_MIN=0.001
06/28 09:40:03 PM | W_MOMENTUM=0.9
06/28 09:40:03 PM | W_WEIGHT_DECAY=0.0003
06/28 09:40:03 PM | WORKERS=4
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Parameters:
06/28 09:41:07 PM | ALPHA_LR=0.0003
06/28 09:41:07 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:41:07 PM | BATCH_SIZE=64
06/28 09:41:07 PM | DATA_PATH=./data/
06/28 09:41:07 PM | DATASET=cifar10
06/28 09:41:07 PM | EPOCHS=50
06/28 09:41:07 PM | GPUS=[0]
06/28 09:41:07 PM | INIT_CHANNELS=16
06/28 09:41:07 PM | LAYERS=8
06/28 09:41:07 PM | NAME=cifar10
06/28 09:41:07 PM | PATH=searchs/cifar10
06/28 09:41:07 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:41:07 PM | PRINT_FREQ=50
06/28 09:41:07 PM | SEED=2
06/28 09:41:07 PM | W_GRAD_CLIP=5.0
06/28 09:41:07 PM | W_LR=0.025
06/28 09:41:07 PM | W_LR_MIN=0.001
06/28 09:41:07 PM | W_MOMENTUM=0.9
06/28 09:41:07 PM | W_WEIGHT_DECAY=0.0003
06/28 09:41:07 PM | WORKERS=4
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Parameters:
06/28 09:50:00 PM | ALPHA_LR=0.0003
06/28 09:50:00 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:50:00 PM | BATCH_SIZE=32
06/28 09:50:00 PM | DATA_PATH=./data/
06/28 09:50:00 PM | DATASET=cifar10
06/28 09:50:00 PM | EPOCHS=50
06/28 09:50:00 PM | GPUS=[0]
06/28 09:50:00 PM | INIT_CHANNELS=16
06/28 09:50:00 PM | LAYERS=8
06/28 09:50:00 PM | NAME=cifar10
06/28 09:50:00 PM | PATH=searchs/cifar10
06/28 09:50:00 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:50:00 PM | PRINT_FREQ=50
06/28 09:50:00 PM | SEED=2
06/28 09:50:00 PM | W_GRAD_CLIP=5.0
06/28 09:50:00 PM | W_LR=0.025
06/28 09:50:00 PM | W_LR_MIN=0.001
06/28 09:50:00 PM | W_MOMENTUM=0.9
06/28 09:50:00 PM | W_WEIGHT_DECAY=0.0003
06/28 09:50:00 PM | WORKERS=4
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:10 PM | Train: [ 1/50] Step 000/781 Loss 2.258 Prec@(1,5) (25.0%, 75.0%)
06/28 09:53:50 PM | Train: [ 1/50] Step 050/781 Loss 2.148 Prec@(1,5) (22.8%, 77.6%)
06/28 09:57:30 PM | Train: [ 1/50] Step 100/781 Loss 2.041 Prec@(1,5) (25.8%, 80.4%)
06/28 10:01:10 PM | Train: [ 1/50] Step 150/781 Loss 1.968 Prec@(1,5) (27.4%, 82.4%)
06/28 10:04:50 PM | Train: [ 1/50] Step 200/781 Loss 1.907 Prec@(1,5) (30.1%, 83.9%)
06/28 10:08:30 PM | Train: [ 1/50] Step 250/781 Loss 1.861 Prec@(1,5) (31.5%, 85.0%)
06/28 10:12:10 PM | Train: [ 1/50] Step 300/781 Loss 1.822 Prec@(1,5) (32.9%, 85.8%)
06/28 10:15:50 PM | Train: [ 1/50] Step 350/781 Loss 1.792 Prec@(1,5) (34.1%, 86.3%)
06/28 10:19:30 PM | Train: [ 1/50] Step 400/781 Loss 1.760 Prec@(1,5) (35.2%, 86.9%)
06/28 10:23:10 PM | Train: [ 1/50] Step 450/781 Loss 1.738 Prec@(1,5) (36.0%, 87.4%)
06/28 10:26:50 PM | Train: [ 1/50] Step 500/781 Loss 1.711 Prec@(1,5) (37.2%, 87.8%)
06/28 10:30:30 PM | Train: [ 1/50] Step 550/781 Loss 1.687 Prec@(1,5) (38.2%, 88.2%)
06/28 10:34:10 PM | Train: [ 1/50] Step 600/781 Loss 1.665 Prec@(1,5) (39.1%, 88.5%)
06/28 10:37:49 PM | Train: [ 1/50] Step 650/781 Loss 1.639 Prec@(1,5) (40.1%, 88.9%)
06/28 10:41:29 PM | Train: [ 1/50] Step 700/781 Loss 1.621 Prec@(1,5) (40.9%, 89.2%)
06/28 10:45:09 PM | Train: [ 1/50] Step 750/781 Loss 1.602 Prec@(1,5) (41.6%, 89.5%)
06/28 10:47:25 PM | Train: [ 1/50] Step 781/781 Loss 1.589 Prec@(1,5) (42.2%, 89.7%)
06/28 10:47:25 PM | Train: [ 1/50] Final Prec@1 42.2200%
06/28 10:47:25 PM | Valid: [ 1/50] Step 000/781 Loss 1.273 Prec@(1,5) (50.0%, 93.8%)
06/28 10:47:36 PM | Valid: [ 1/50] Step 050/781 Loss 1.343 Prec@(1,5) (52.2%, 94.2%)
06/28 10:47:47 PM | Valid: [ 1/50] Step 100/781 Loss 1.340 Prec@(1,5) (52.0%, 94.3%)
06/28 10:47:57 PM | Valid: [ 1/50] Step 150/781 Loss 1.340 Prec@(1,5) (51.9%, 94.4%)
06/28 10:48:08 PM | Valid: [ 1/50] Step 200/781 Loss 1.345 Prec@(1,5) (52.1%, 94.0%)
06/28 10:48:19 PM | Valid: [ 1/50] Step 250/781 Loss 1.330 Prec@(1,5) (52.7%, 94.2%)
06/28 10:48:30 PM | Valid: [ 1/50] Step 300/781 Loss 1.327 Prec@(1,5) (53.2%, 94.2%)
06/28 10:48:41 PM | Valid: [ 1/50] Step 350/781 Loss 1.337 Prec@(1,5) (53.0%, 94.2%)
06/28 10:48:53 PM | Valid: [ 1/50] Step 400/781 Loss 1.336 Prec@(1,5) (53.0%, 94.2%)
06/28 10:49:04 PM | Valid: [ 1/50] Step 450/781 Loss 1.338 Prec@(1,5) (53.0%, 94.1%)
06/28 10:49:14 PM | Valid: [ 1/50] Step 500/781 Loss 1.339 Prec@(1,5) (52.9%, 94.1%)
06/28 10:49:25 PM | Valid: [ 1/50] Step 550/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:49:36 PM | Valid: [ 1/50] Step 600/781 Loss 1.343 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:48 PM | Valid: [ 1/50] Step 650/781 Loss 1.342 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:59 PM | Valid: [ 1/50] Step 700/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:50:09 PM | Valid: [ 1/50] Step 750/781 Loss 1.338 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Step 781/781 Loss 1.337 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Final Prec@1 53.1440%
06/28 10:50:16 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1266, 0.1190, 0.1251, 0.1273, 0.1255, 0.1271, 0.1250, 0.1243],
        [0.1229, 0.1182, 0.1220, 0.1250, 0.1274, 0.1270, 0.1263, 0.1310]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1264, 0.1190, 0.1232, 0.1264, 0.1272, 0.1238, 0.1269, 0.1271],
        [0.1239, 0.1180, 0.1220, 0.1261, 0.1272, 0.1268, 0.1275, 0.1284],
        [0.1191, 0.1158, 0.1218, 0.1256, 0.1250, 0.1288, 0.1301, 0.1338]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1260, 0.1177, 0.1221, 0.1274, 0.1278, 0.1252, 0.1264, 0.1276],
        [0.1224, 0.1171, 0.1201, 0.1276, 0.1289, 0.1282, 0.1275, 0.1283],
        [0.1186, 0.1157, 0.1213, 0.1286, 0.1283, 0.1280, 0.1268, 0.1328],
        [0.1176, 0.1145, 0.1187, 0.1296, 0.1269, 0.1296, 0.1294, 0.1336]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1243, 0.1172, 0.1205, 0.1278, 0.1265, 0.1268, 0.1276, 0.1294],
        [0.1227, 0.1178, 0.1208, 0.1273, 0.1291, 0.1261, 0.1272, 0.1289],
        [0.1171, 0.1145, 0.1197, 0.1268, 0.1286, 0.1305, 0.1285, 0.1344],
        [0.1161, 0.1138, 0.1168, 0.1319, 0.1284, 0.1299, 0.1301, 0.1330],
        [0.1161, 0.1139, 0.1165, 0.1289, 0.1310, 0.1288, 0.1318, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1244, 0.1242, 0.1252, 0.1275, 0.1226, 0.1252, 0.1229],
        [0.1256, 0.1207, 0.1265, 0.1238, 0.1257, 0.1255, 0.1256, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1245, 0.1255, 0.1264, 0.1256, 0.1232, 0.1229, 0.1228],
        [0.1271, 0.1223, 0.1258, 0.1255, 0.1249, 0.1248, 0.1227, 0.1269],
        [0.1255, 0.1199, 0.1255, 0.1251, 0.1253, 0.1257, 0.1272, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1282, 0.1233, 0.1270, 0.1258, 0.1250, 0.1244, 0.1236, 0.1227],
        [0.1262, 0.1217, 0.1237, 0.1248, 0.1287, 0.1277, 0.1233, 0.1238],
        [0.1245, 0.1184, 0.1239, 0.1243, 0.1278, 0.1253, 0.1295, 0.1263],
        [0.1217, 0.1184, 0.1243, 0.1244, 0.1290, 0.1284, 0.1254, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1247, 0.1232, 0.1270, 0.1241, 0.1247, 0.1234, 0.1239],
        [0.1268, 0.1221, 0.1258, 0.1246, 0.1243, 0.1264, 0.1261, 0.1240],
        [0.1244, 0.1201, 0.1250, 0.1242, 0.1290, 0.1239, 0.1277, 0.1257],
        [0.1220, 0.1187, 0.1245, 0.1287, 0.1272, 0.1257, 0.1254, 0.1279],
        [0.1225, 0.1204, 0.1245, 0.1257, 0.1253, 0.1266, 0.1281, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 10:50:22 PM | Train: [ 2/50] Step 000/781 Loss 1.193 Prec@(1,5) (56.2%, 100.0%)
06/28 10:54:06 PM | Train: [ 2/50] Step 050/781 Loss 1.307 Prec@(1,5) (53.6%, 93.6%)
06/28 10:57:50 PM | Train: [ 2/50] Step 100/781 Loss 1.253 Prec@(1,5) (55.6%, 94.3%)
06/28 11:01:34 PM | Train: [ 2/50] Step 150/781 Loss 1.245 Prec@(1,5) (55.5%, 94.6%)
06/28 11:05:18 PM | Train: [ 2/50] Step 200/781 Loss 1.241 Prec@(1,5) (55.4%, 94.4%)
06/28 11:09:02 PM | Train: [ 2/50] Step 250/781 Loss 1.235 Prec@(1,5) (55.6%, 94.6%)
06/28 11:12:46 PM | Train: [ 2/50] Step 300/781 Loss 1.224 Prec@(1,5) (56.0%, 94.8%)
06/28 11:16:30 PM | Train: [ 2/50] Step 350/781 Loss 1.210 Prec@(1,5) (56.6%, 95.0%)
06/28 11:20:15 PM | Train: [ 2/50] Step 400/781 Loss 1.198 Prec@(1,5) (56.9%, 95.1%)
06/28 11:23:59 PM | Train: [ 2/50] Step 450/781 Loss 1.196 Prec@(1,5) (57.1%, 95.2%)
06/28 11:27:41 PM | Train: [ 2/50] Step 500/781 Loss 1.186 Prec@(1,5) (57.6%, 95.2%)
06/28 11:31:21 PM | Train: [ 2/50] Step 550/781 Loss 1.180 Prec@(1,5) (58.0%, 95.2%)
06/28 11:35:01 PM | Train: [ 2/50] Step 600/781 Loss 1.172 Prec@(1,5) (58.3%, 95.3%)
06/28 11:38:42 PM | Train: [ 2/50] Step 650/781 Loss 1.159 Prec@(1,5) (58.8%, 95.4%)
06/28 11:42:22 PM | Train: [ 2/50] Step 700/781 Loss 1.152 Prec@(1,5) (59.0%, 95.5%)
06/28 11:46:02 PM | Train: [ 2/50] Step 750/781 Loss 1.144 Prec@(1,5) (59.4%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Step 781/781 Loss 1.140 Prec@(1,5) (59.5%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Final Prec@1 59.5280%
06/28 11:48:18 PM | Valid: [ 2/50] Step 000/781 Loss 0.708 Prec@(1,5) (81.2%, 96.9%)
06/28 11:48:28 PM | Valid: [ 2/50] Step 050/781 Loss 1.032 Prec@(1,5) (62.5%, 96.9%)
06/28 11:48:39 PM | Valid: [ 2/50] Step 100/781 Loss 1.023 Prec@(1,5) (63.1%, 96.7%)
06/28 11:48:49 PM | Valid: [ 2/50] Step 150/781 Loss 1.024 Prec@(1,5) (63.4%, 96.6%)
06/28 11:49:00 PM | Valid: [ 2/50] Step 200/781 Loss 1.037 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:11 PM | Valid: [ 2/50] Step 250/781 Loss 1.044 Prec@(1,5) (63.2%, 96.3%)
06/28 11:49:21 PM | Valid: [ 2/50] Step 300/781 Loss 1.037 Prec@(1,5) (63.5%, 96.3%)
06/28 11:49:32 PM | Valid: [ 2/50] Step 350/781 Loss 1.043 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:43 PM | Valid: [ 2/50] Step 400/781 Loss 1.046 Prec@(1,5) (63.3%, 96.3%)
06/28 11:49:53 PM | Valid: [ 2/50] Step 450/781 Loss 1.047 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:04 PM | Valid: [ 2/50] Step 500/781 Loss 1.048 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:15 PM | Valid: [ 2/50] Step 550/781 Loss 1.050 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:25 PM | Valid: [ 2/50] Step 600/781 Loss 1.054 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:36 PM | Valid: [ 2/50] Step 650/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:47 PM | Valid: [ 2/50] Step 700/781 Loss 1.047 Prec@(1,5) (63.2%, 96.4%)
06/28 11:50:57 PM | Valid: [ 2/50] Step 750/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Step 781/781 Loss 1.050 Prec@(1,5) (63.2%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Final Prec@1 63.2200%
06/28 11:51:04 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 3), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1279, 0.1139, 0.1252, 0.1291, 0.1260, 0.1287, 0.1248, 0.1244],
        [0.1231, 0.1143, 0.1212, 0.1247, 0.1270, 0.1297, 0.1260, 0.1340]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1280, 0.1138, 0.1210, 0.1273, 0.1292, 0.1255, 0.1274, 0.1280],
        [0.1246, 0.1147, 0.1224, 0.1274, 0.1279, 0.1257, 0.1267, 0.1306],
        [0.1165, 0.1093, 0.1189, 0.1276, 0.1249, 0.1284, 0.1366, 0.1378]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1124, 0.1209, 0.1279, 0.1295, 0.1246, 0.1263, 0.1299],
        [0.1216, 0.1119, 0.1178, 0.1303, 0.1295, 0.1312, 0.1269, 0.1307],
        [0.1163, 0.1094, 0.1193, 0.1296, 0.1303, 0.1276, 0.1291, 0.1383],
        [0.1148, 0.1074, 0.1140, 0.1346, 0.1289, 0.1320, 0.1308, 0.1375]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1259, 0.1126, 0.1193, 0.1286, 0.1279, 0.1255, 0.1284, 0.1318],
        [0.1223, 0.1134, 0.1189, 0.1276, 0.1306, 0.1296, 0.1265, 0.1311],
        [0.1143, 0.1080, 0.1168, 0.1290, 0.1299, 0.1316, 0.1298, 0.1407],
        [0.1131, 0.1074, 0.1127, 0.1347, 0.1294, 0.1323, 0.1318, 0.1386],
        [0.1123, 0.1071, 0.1113, 0.1323, 0.1353, 0.1318, 0.1319, 0.1379]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1333, 0.1246, 0.1247, 0.1267, 0.1277, 0.1229, 0.1193, 0.1209],
        [0.1281, 0.1183, 0.1264, 0.1261, 0.1259, 0.1229, 0.1231, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1346, 0.1252, 0.1244, 0.1256, 0.1250, 0.1228, 0.1227, 0.1196],
        [0.1288, 0.1194, 0.1241, 0.1254, 0.1271, 0.1254, 0.1217, 0.1280],
        [0.1277, 0.1141, 0.1249, 0.1242, 0.1261, 0.1255, 0.1307, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1319, 0.1214, 0.1278, 0.1252, 0.1261, 0.1228, 0.1226, 0.1223],
        [0.1290, 0.1198, 0.1249, 0.1235, 0.1278, 0.1273, 0.1231, 0.1246],
        [0.1254, 0.1108, 0.1215, 0.1241, 0.1324, 0.1240, 0.1337, 0.1281],
        [0.1200, 0.1108, 0.1202, 0.1248, 0.1341, 0.1354, 0.1279, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1238, 0.1205, 0.1296, 0.1271, 0.1231, 0.1212, 0.1224],
        [0.1286, 0.1201, 0.1244, 0.1250, 0.1241, 0.1262, 0.1247, 0.1270],
        [0.1246, 0.1132, 0.1232, 0.1260, 0.1307, 0.1228, 0.1324, 0.1272],
        [0.1195, 0.1117, 0.1215, 0.1313, 0.1306, 0.1285, 0.1291, 0.1278],
        [0.1213, 0.1146, 0.1232, 0.1263, 0.1255, 0.1276, 0.1313, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 11:51:09 PM | Train: [ 3/50] Step 000/781 Loss 1.001 Prec@(1,5) (53.1%, 100.0%)
06/28 11:54:49 PM | Train: [ 3/50] Step 050/781 Loss 0.970 Prec@(1,5) (67.5%, 96.8%)
06/28 11:58:29 PM | Train: [ 3/50] Step 100/781 Loss 0.971 Prec@(1,5) (66.7%, 96.9%)
06/29 12:02:09 AM | Train: [ 3/50] Step 150/781 Loss 0.987 Prec@(1,5) (65.8%, 96.9%)
06/29 12:05:49 AM | Train: [ 3/50] Step 200/781 Loss 0.989 Prec@(1,5) (65.7%, 96.8%)
06/29 12:09:29 AM | Train: [ 3/50] Step 250/781 Loss 0.983 Prec@(1,5) (65.9%, 96.8%)
06/29 12:13:09 AM | Train: [ 3/50] Step 300/781 Loss 0.974 Prec@(1,5) (66.3%, 96.8%)
06/29 12:16:49 AM | Train: [ 3/50] Step 350/781 Loss 0.973 Prec@(1,5) (66.4%, 96.8%)
06/29 12:20:29 AM | Train: [ 3/50] Step 400/781 Loss 0.969 Prec@(1,5) (66.6%, 96.9%)
06/29 12:24:10 AM | Train: [ 3/50] Step 450/781 Loss 0.968 Prec@(1,5) (66.7%, 96.9%)
06/29 12:27:50 AM | Train: [ 3/50] Step 500/781 Loss 0.958 Prec@(1,5) (67.0%, 96.9%)
06/29 12:31:30 AM | Train: [ 3/50] Step 550/781 Loss 0.956 Prec@(1,5) (66.9%, 97.0%)
