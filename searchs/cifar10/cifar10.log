06/28 09:22:15 PM | 
06/28 09:22:15 PM | Parameters:
06/28 09:22:15 PM | ALPHA_LR=0.0003
06/28 09:22:15 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:22:15 PM | BATCH_SIZE=64
06/28 09:22:15 PM | DATA_PATH=./data/
06/28 09:22:15 PM | DATASET=cifar10
06/28 09:22:15 PM | EPOCHS=50
06/28 09:22:15 PM | GPUS=[0]
06/28 09:22:15 PM | INIT_CHANNELS=16
06/28 09:22:15 PM | LAYERS=8
06/28 09:22:15 PM | NAME=cifar10
06/28 09:22:15 PM | PATH=searchs/cifar10
06/28 09:22:15 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:22:15 PM | PRINT_FREQ=50
06/28 09:22:15 PM | SEED=2
06/28 09:22:15 PM | W_GRAD_CLIP=5.0
06/28 09:22:15 PM | W_LR=0.025
06/28 09:22:15 PM | W_LR_MIN=0.001
06/28 09:22:15 PM | W_MOMENTUM=0.9
06/28 09:22:15 PM | W_WEIGHT_DECAY=0.0003
06/28 09:22:15 PM | WORKERS=4
06/28 09:22:15 PM | 
06/28 09:22:15 PM | Logger is set - training start
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Parameters:
06/28 09:28:51 PM | ALPHA_LR=0.0003
06/28 09:28:51 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:28:51 PM | BATCH_SIZE=64
06/28 09:28:51 PM | DATA_PATH=./data/
06/28 09:28:51 PM | DATASET=cifar10
06/28 09:28:51 PM | EPOCHS=50
06/28 09:28:51 PM | GPUS=[0]
06/28 09:28:51 PM | INIT_CHANNELS=16
06/28 09:28:51 PM | LAYERS=8
06/28 09:28:51 PM | NAME=cifar10
06/28 09:28:51 PM | PATH=searchs/cifar10
06/28 09:28:51 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:28:51 PM | PRINT_FREQ=50
06/28 09:28:51 PM | SEED=2
06/28 09:28:51 PM | W_GRAD_CLIP=5.0
06/28 09:28:51 PM | W_LR=0.025
06/28 09:28:51 PM | W_LR_MIN=0.001
06/28 09:28:51 PM | W_MOMENTUM=0.9
06/28 09:28:51 PM | W_WEIGHT_DECAY=0.0003
06/28 09:28:51 PM | WORKERS=4
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Logger is set - training start
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Parameters:
06/28 09:30:37 PM | ALPHA_LR=0.0003
06/28 09:30:37 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:30:37 PM | BATCH_SIZE=64
06/28 09:30:37 PM | DATA_PATH=./data/
06/28 09:30:37 PM | DATASET=cifar10
06/28 09:30:37 PM | EPOCHS=50
06/28 09:30:37 PM | GPUS=[0]
06/28 09:30:37 PM | INIT_CHANNELS=16
06/28 09:30:37 PM | LAYERS=8
06/28 09:30:37 PM | NAME=cifar10
06/28 09:30:37 PM | PATH=searchs/cifar10
06/28 09:30:37 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:30:37 PM | PRINT_FREQ=50
06/28 09:30:37 PM | SEED=2
06/28 09:30:37 PM | W_GRAD_CLIP=5.0
06/28 09:30:37 PM | W_LR=0.025
06/28 09:30:37 PM | W_LR_MIN=0.001
06/28 09:30:37 PM | W_MOMENTUM=0.9
06/28 09:30:37 PM | W_WEIGHT_DECAY=0.0003
06/28 09:30:37 PM | WORKERS=4
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Parameters:
06/28 09:31:11 PM | ALPHA_LR=0.0003
06/28 09:31:11 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:11 PM | BATCH_SIZE=64
06/28 09:31:11 PM | DATA_PATH=./data/
06/28 09:31:11 PM | DATASET=cifar10
06/28 09:31:11 PM | EPOCHS=50
06/28 09:31:11 PM | GPUS=[0]
06/28 09:31:11 PM | INIT_CHANNELS=16
06/28 09:31:11 PM | LAYERS=8
06/28 09:31:11 PM | NAME=cifar10
06/28 09:31:11 PM | PATH=searchs/cifar10
06/28 09:31:11 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:11 PM | PRINT_FREQ=50
06/28 09:31:11 PM | SEED=2
06/28 09:31:11 PM | W_GRAD_CLIP=5.0
06/28 09:31:11 PM | W_LR=0.025
06/28 09:31:11 PM | W_LR_MIN=0.001
06/28 09:31:11 PM | W_MOMENTUM=0.9
06/28 09:31:11 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:11 PM | WORKERS=4
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Parameters:
06/28 09:31:44 PM | ALPHA_LR=0.0003
06/28 09:31:44 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:44 PM | BATCH_SIZE=64
06/28 09:31:44 PM | DATA_PATH=./data/
06/28 09:31:44 PM | DATASET=cifar10
06/28 09:31:44 PM | EPOCHS=50
06/28 09:31:44 PM | GPUS=[0]
06/28 09:31:44 PM | INIT_CHANNELS=16
06/28 09:31:44 PM | LAYERS=8
06/28 09:31:44 PM | NAME=cifar10
06/28 09:31:44 PM | PATH=searchs/cifar10
06/28 09:31:44 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:44 PM | PRINT_FREQ=50
06/28 09:31:44 PM | SEED=2
06/28 09:31:44 PM | W_GRAD_CLIP=5.0
06/28 09:31:44 PM | W_LR=0.025
06/28 09:31:44 PM | W_LR_MIN=0.001
06/28 09:31:44 PM | W_MOMENTUM=0.9
06/28 09:31:44 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:44 PM | WORKERS=4
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Parameters:
06/28 09:37:36 PM | ALPHA_LR=0.0003
06/28 09:37:36 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:37:36 PM | BATCH_SIZE=64
06/28 09:37:36 PM | DATA_PATH=./data/
06/28 09:37:36 PM | DATASET=cifar10
06/28 09:37:36 PM | EPOCHS=50
06/28 09:37:36 PM | GPUS=[0]
06/28 09:37:36 PM | INIT_CHANNELS=16
06/28 09:37:36 PM | LAYERS=8
06/28 09:37:36 PM | NAME=cifar10
06/28 09:37:36 PM | PATH=searchs/cifar10
06/28 09:37:36 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:37:36 PM | PRINT_FREQ=50
06/28 09:37:36 PM | SEED=2
06/28 09:37:36 PM | W_GRAD_CLIP=5.0
06/28 09:37:36 PM | W_LR=0.025
06/28 09:37:36 PM | W_LR_MIN=0.001
06/28 09:37:36 PM | W_MOMENTUM=0.9
06/28 09:37:36 PM | W_WEIGHT_DECAY=0.0003
06/28 09:37:36 PM | WORKERS=4
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Parameters:
06/28 09:38:10 PM | ALPHA_LR=0.0003
06/28 09:38:10 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:38:10 PM | BATCH_SIZE=64
06/28 09:38:10 PM | DATA_PATH=./data/
06/28 09:38:10 PM | DATASET=cifar10
06/28 09:38:10 PM | EPOCHS=50
06/28 09:38:10 PM | GPUS=[0]
06/28 09:38:10 PM | INIT_CHANNELS=16
06/28 09:38:10 PM | LAYERS=8
06/28 09:38:10 PM | NAME=cifar10
06/28 09:38:10 PM | PATH=searchs/cifar10
06/28 09:38:10 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:38:10 PM | PRINT_FREQ=50
06/28 09:38:10 PM | SEED=2
06/28 09:38:10 PM | W_GRAD_CLIP=5.0
06/28 09:38:10 PM | W_LR=0.025
06/28 09:38:10 PM | W_LR_MIN=0.001
06/28 09:38:10 PM | W_MOMENTUM=0.9
06/28 09:38:10 PM | W_WEIGHT_DECAY=0.0003
06/28 09:38:10 PM | WORKERS=4
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Parameters:
06/28 09:40:03 PM | ALPHA_LR=0.0003
06/28 09:40:03 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:40:03 PM | BATCH_SIZE=64
06/28 09:40:03 PM | DATA_PATH=./data/
06/28 09:40:03 PM | DATASET=cifar10
06/28 09:40:03 PM | EPOCHS=50
06/28 09:40:03 PM | GPUS=[0]
06/28 09:40:03 PM | INIT_CHANNELS=16
06/28 09:40:03 PM | LAYERS=8
06/28 09:40:03 PM | NAME=cifar10
06/28 09:40:03 PM | PATH=searchs/cifar10
06/28 09:40:03 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:40:03 PM | PRINT_FREQ=50
06/28 09:40:03 PM | SEED=2
06/28 09:40:03 PM | W_GRAD_CLIP=5.0
06/28 09:40:03 PM | W_LR=0.025
06/28 09:40:03 PM | W_LR_MIN=0.001
06/28 09:40:03 PM | W_MOMENTUM=0.9
06/28 09:40:03 PM | W_WEIGHT_DECAY=0.0003
06/28 09:40:03 PM | WORKERS=4
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Parameters:
06/28 09:41:07 PM | ALPHA_LR=0.0003
06/28 09:41:07 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:41:07 PM | BATCH_SIZE=64
06/28 09:41:07 PM | DATA_PATH=./data/
06/28 09:41:07 PM | DATASET=cifar10
06/28 09:41:07 PM | EPOCHS=50
06/28 09:41:07 PM | GPUS=[0]
06/28 09:41:07 PM | INIT_CHANNELS=16
06/28 09:41:07 PM | LAYERS=8
06/28 09:41:07 PM | NAME=cifar10
06/28 09:41:07 PM | PATH=searchs/cifar10
06/28 09:41:07 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:41:07 PM | PRINT_FREQ=50
06/28 09:41:07 PM | SEED=2
06/28 09:41:07 PM | W_GRAD_CLIP=5.0
06/28 09:41:07 PM | W_LR=0.025
06/28 09:41:07 PM | W_LR_MIN=0.001
06/28 09:41:07 PM | W_MOMENTUM=0.9
06/28 09:41:07 PM | W_WEIGHT_DECAY=0.0003
06/28 09:41:07 PM | WORKERS=4
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Parameters:
06/28 09:50:00 PM | ALPHA_LR=0.0003
06/28 09:50:00 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:50:00 PM | BATCH_SIZE=32
06/28 09:50:00 PM | DATA_PATH=./data/
06/28 09:50:00 PM | DATASET=cifar10
06/28 09:50:00 PM | EPOCHS=50
06/28 09:50:00 PM | GPUS=[0]
06/28 09:50:00 PM | INIT_CHANNELS=16
06/28 09:50:00 PM | LAYERS=8
06/28 09:50:00 PM | NAME=cifar10
06/28 09:50:00 PM | PATH=searchs/cifar10
06/28 09:50:00 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:50:00 PM | PRINT_FREQ=50
06/28 09:50:00 PM | SEED=2
06/28 09:50:00 PM | W_GRAD_CLIP=5.0
06/28 09:50:00 PM | W_LR=0.025
06/28 09:50:00 PM | W_LR_MIN=0.001
06/28 09:50:00 PM | W_MOMENTUM=0.9
06/28 09:50:00 PM | W_WEIGHT_DECAY=0.0003
06/28 09:50:00 PM | WORKERS=4
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:10 PM | Train: [ 1/50] Step 000/781 Loss 2.258 Prec@(1,5) (25.0%, 75.0%)
06/28 09:53:50 PM | Train: [ 1/50] Step 050/781 Loss 2.148 Prec@(1,5) (22.8%, 77.6%)
06/28 09:57:30 PM | Train: [ 1/50] Step 100/781 Loss 2.041 Prec@(1,5) (25.8%, 80.4%)
06/28 10:01:10 PM | Train: [ 1/50] Step 150/781 Loss 1.968 Prec@(1,5) (27.4%, 82.4%)
06/28 10:04:50 PM | Train: [ 1/50] Step 200/781 Loss 1.907 Prec@(1,5) (30.1%, 83.9%)
06/28 10:08:30 PM | Train: [ 1/50] Step 250/781 Loss 1.861 Prec@(1,5) (31.5%, 85.0%)
06/28 10:12:10 PM | Train: [ 1/50] Step 300/781 Loss 1.822 Prec@(1,5) (32.9%, 85.8%)
06/28 10:15:50 PM | Train: [ 1/50] Step 350/781 Loss 1.792 Prec@(1,5) (34.1%, 86.3%)
06/28 10:19:30 PM | Train: [ 1/50] Step 400/781 Loss 1.760 Prec@(1,5) (35.2%, 86.9%)
06/28 10:23:10 PM | Train: [ 1/50] Step 450/781 Loss 1.738 Prec@(1,5) (36.0%, 87.4%)
06/28 10:26:50 PM | Train: [ 1/50] Step 500/781 Loss 1.711 Prec@(1,5) (37.2%, 87.8%)
06/28 10:30:30 PM | Train: [ 1/50] Step 550/781 Loss 1.687 Prec@(1,5) (38.2%, 88.2%)
06/28 10:34:10 PM | Train: [ 1/50] Step 600/781 Loss 1.665 Prec@(1,5) (39.1%, 88.5%)
06/28 10:37:49 PM | Train: [ 1/50] Step 650/781 Loss 1.639 Prec@(1,5) (40.1%, 88.9%)
06/28 10:41:29 PM | Train: [ 1/50] Step 700/781 Loss 1.621 Prec@(1,5) (40.9%, 89.2%)
06/28 10:45:09 PM | Train: [ 1/50] Step 750/781 Loss 1.602 Prec@(1,5) (41.6%, 89.5%)
06/28 10:47:25 PM | Train: [ 1/50] Step 781/781 Loss 1.589 Prec@(1,5) (42.2%, 89.7%)
06/28 10:47:25 PM | Train: [ 1/50] Final Prec@1 42.2200%
06/28 10:47:25 PM | Valid: [ 1/50] Step 000/781 Loss 1.273 Prec@(1,5) (50.0%, 93.8%)
06/28 10:47:36 PM | Valid: [ 1/50] Step 050/781 Loss 1.343 Prec@(1,5) (52.2%, 94.2%)
06/28 10:47:47 PM | Valid: [ 1/50] Step 100/781 Loss 1.340 Prec@(1,5) (52.0%, 94.3%)
06/28 10:47:57 PM | Valid: [ 1/50] Step 150/781 Loss 1.340 Prec@(1,5) (51.9%, 94.4%)
06/28 10:48:08 PM | Valid: [ 1/50] Step 200/781 Loss 1.345 Prec@(1,5) (52.1%, 94.0%)
06/28 10:48:19 PM | Valid: [ 1/50] Step 250/781 Loss 1.330 Prec@(1,5) (52.7%, 94.2%)
06/28 10:48:30 PM | Valid: [ 1/50] Step 300/781 Loss 1.327 Prec@(1,5) (53.2%, 94.2%)
06/28 10:48:41 PM | Valid: [ 1/50] Step 350/781 Loss 1.337 Prec@(1,5) (53.0%, 94.2%)
06/28 10:48:53 PM | Valid: [ 1/50] Step 400/781 Loss 1.336 Prec@(1,5) (53.0%, 94.2%)
06/28 10:49:04 PM | Valid: [ 1/50] Step 450/781 Loss 1.338 Prec@(1,5) (53.0%, 94.1%)
06/28 10:49:14 PM | Valid: [ 1/50] Step 500/781 Loss 1.339 Prec@(1,5) (52.9%, 94.1%)
06/28 10:49:25 PM | Valid: [ 1/50] Step 550/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:49:36 PM | Valid: [ 1/50] Step 600/781 Loss 1.343 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:48 PM | Valid: [ 1/50] Step 650/781 Loss 1.342 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:59 PM | Valid: [ 1/50] Step 700/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:50:09 PM | Valid: [ 1/50] Step 750/781 Loss 1.338 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Step 781/781 Loss 1.337 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Final Prec@1 53.1440%
06/28 10:50:16 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1266, 0.1190, 0.1251, 0.1273, 0.1255, 0.1271, 0.1250, 0.1243],
        [0.1229, 0.1182, 0.1220, 0.1250, 0.1274, 0.1270, 0.1263, 0.1310]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1264, 0.1190, 0.1232, 0.1264, 0.1272, 0.1238, 0.1269, 0.1271],
        [0.1239, 0.1180, 0.1220, 0.1261, 0.1272, 0.1268, 0.1275, 0.1284],
        [0.1191, 0.1158, 0.1218, 0.1256, 0.1250, 0.1288, 0.1301, 0.1338]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1260, 0.1177, 0.1221, 0.1274, 0.1278, 0.1252, 0.1264, 0.1276],
        [0.1224, 0.1171, 0.1201, 0.1276, 0.1289, 0.1282, 0.1275, 0.1283],
        [0.1186, 0.1157, 0.1213, 0.1286, 0.1283, 0.1280, 0.1268, 0.1328],
        [0.1176, 0.1145, 0.1187, 0.1296, 0.1269, 0.1296, 0.1294, 0.1336]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1243, 0.1172, 0.1205, 0.1278, 0.1265, 0.1268, 0.1276, 0.1294],
        [0.1227, 0.1178, 0.1208, 0.1273, 0.1291, 0.1261, 0.1272, 0.1289],
        [0.1171, 0.1145, 0.1197, 0.1268, 0.1286, 0.1305, 0.1285, 0.1344],
        [0.1161, 0.1138, 0.1168, 0.1319, 0.1284, 0.1299, 0.1301, 0.1330],
        [0.1161, 0.1139, 0.1165, 0.1289, 0.1310, 0.1288, 0.1318, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1244, 0.1242, 0.1252, 0.1275, 0.1226, 0.1252, 0.1229],
        [0.1256, 0.1207, 0.1265, 0.1238, 0.1257, 0.1255, 0.1256, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1245, 0.1255, 0.1264, 0.1256, 0.1232, 0.1229, 0.1228],
        [0.1271, 0.1223, 0.1258, 0.1255, 0.1249, 0.1248, 0.1227, 0.1269],
        [0.1255, 0.1199, 0.1255, 0.1251, 0.1253, 0.1257, 0.1272, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1282, 0.1233, 0.1270, 0.1258, 0.1250, 0.1244, 0.1236, 0.1227],
        [0.1262, 0.1217, 0.1237, 0.1248, 0.1287, 0.1277, 0.1233, 0.1238],
        [0.1245, 0.1184, 0.1239, 0.1243, 0.1278, 0.1253, 0.1295, 0.1263],
        [0.1217, 0.1184, 0.1243, 0.1244, 0.1290, 0.1284, 0.1254, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1247, 0.1232, 0.1270, 0.1241, 0.1247, 0.1234, 0.1239],
        [0.1268, 0.1221, 0.1258, 0.1246, 0.1243, 0.1264, 0.1261, 0.1240],
        [0.1244, 0.1201, 0.1250, 0.1242, 0.1290, 0.1239, 0.1277, 0.1257],
        [0.1220, 0.1187, 0.1245, 0.1287, 0.1272, 0.1257, 0.1254, 0.1279],
        [0.1225, 0.1204, 0.1245, 0.1257, 0.1253, 0.1266, 0.1281, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 10:50:22 PM | Train: [ 2/50] Step 000/781 Loss 1.193 Prec@(1,5) (56.2%, 100.0%)
06/28 10:54:06 PM | Train: [ 2/50] Step 050/781 Loss 1.307 Prec@(1,5) (53.6%, 93.6%)
06/28 10:57:50 PM | Train: [ 2/50] Step 100/781 Loss 1.253 Prec@(1,5) (55.6%, 94.3%)
06/28 11:01:34 PM | Train: [ 2/50] Step 150/781 Loss 1.245 Prec@(1,5) (55.5%, 94.6%)
06/28 11:05:18 PM | Train: [ 2/50] Step 200/781 Loss 1.241 Prec@(1,5) (55.4%, 94.4%)
06/28 11:09:02 PM | Train: [ 2/50] Step 250/781 Loss 1.235 Prec@(1,5) (55.6%, 94.6%)
06/28 11:12:46 PM | Train: [ 2/50] Step 300/781 Loss 1.224 Prec@(1,5) (56.0%, 94.8%)
06/28 11:16:30 PM | Train: [ 2/50] Step 350/781 Loss 1.210 Prec@(1,5) (56.6%, 95.0%)
06/28 11:20:15 PM | Train: [ 2/50] Step 400/781 Loss 1.198 Prec@(1,5) (56.9%, 95.1%)
06/28 11:23:59 PM | Train: [ 2/50] Step 450/781 Loss 1.196 Prec@(1,5) (57.1%, 95.2%)
06/28 11:27:41 PM | Train: [ 2/50] Step 500/781 Loss 1.186 Prec@(1,5) (57.6%, 95.2%)
06/28 11:31:21 PM | Train: [ 2/50] Step 550/781 Loss 1.180 Prec@(1,5) (58.0%, 95.2%)
06/28 11:35:01 PM | Train: [ 2/50] Step 600/781 Loss 1.172 Prec@(1,5) (58.3%, 95.3%)
06/28 11:38:42 PM | Train: [ 2/50] Step 650/781 Loss 1.159 Prec@(1,5) (58.8%, 95.4%)
06/28 11:42:22 PM | Train: [ 2/50] Step 700/781 Loss 1.152 Prec@(1,5) (59.0%, 95.5%)
06/28 11:46:02 PM | Train: [ 2/50] Step 750/781 Loss 1.144 Prec@(1,5) (59.4%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Step 781/781 Loss 1.140 Prec@(1,5) (59.5%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Final Prec@1 59.5280%
06/28 11:48:18 PM | Valid: [ 2/50] Step 000/781 Loss 0.708 Prec@(1,5) (81.2%, 96.9%)
06/28 11:48:28 PM | Valid: [ 2/50] Step 050/781 Loss 1.032 Prec@(1,5) (62.5%, 96.9%)
06/28 11:48:39 PM | Valid: [ 2/50] Step 100/781 Loss 1.023 Prec@(1,5) (63.1%, 96.7%)
06/28 11:48:49 PM | Valid: [ 2/50] Step 150/781 Loss 1.024 Prec@(1,5) (63.4%, 96.6%)
06/28 11:49:00 PM | Valid: [ 2/50] Step 200/781 Loss 1.037 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:11 PM | Valid: [ 2/50] Step 250/781 Loss 1.044 Prec@(1,5) (63.2%, 96.3%)
06/28 11:49:21 PM | Valid: [ 2/50] Step 300/781 Loss 1.037 Prec@(1,5) (63.5%, 96.3%)
06/28 11:49:32 PM | Valid: [ 2/50] Step 350/781 Loss 1.043 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:43 PM | Valid: [ 2/50] Step 400/781 Loss 1.046 Prec@(1,5) (63.3%, 96.3%)
06/28 11:49:53 PM | Valid: [ 2/50] Step 450/781 Loss 1.047 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:04 PM | Valid: [ 2/50] Step 500/781 Loss 1.048 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:15 PM | Valid: [ 2/50] Step 550/781 Loss 1.050 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:25 PM | Valid: [ 2/50] Step 600/781 Loss 1.054 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:36 PM | Valid: [ 2/50] Step 650/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:47 PM | Valid: [ 2/50] Step 700/781 Loss 1.047 Prec@(1,5) (63.2%, 96.4%)
06/28 11:50:57 PM | Valid: [ 2/50] Step 750/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Step 781/781 Loss 1.050 Prec@(1,5) (63.2%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Final Prec@1 63.2200%
06/28 11:51:04 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 3), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1279, 0.1139, 0.1252, 0.1291, 0.1260, 0.1287, 0.1248, 0.1244],
        [0.1231, 0.1143, 0.1212, 0.1247, 0.1270, 0.1297, 0.1260, 0.1340]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1280, 0.1138, 0.1210, 0.1273, 0.1292, 0.1255, 0.1274, 0.1280],
        [0.1246, 0.1147, 0.1224, 0.1274, 0.1279, 0.1257, 0.1267, 0.1306],
        [0.1165, 0.1093, 0.1189, 0.1276, 0.1249, 0.1284, 0.1366, 0.1378]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1124, 0.1209, 0.1279, 0.1295, 0.1246, 0.1263, 0.1299],
        [0.1216, 0.1119, 0.1178, 0.1303, 0.1295, 0.1312, 0.1269, 0.1307],
        [0.1163, 0.1094, 0.1193, 0.1296, 0.1303, 0.1276, 0.1291, 0.1383],
        [0.1148, 0.1074, 0.1140, 0.1346, 0.1289, 0.1320, 0.1308, 0.1375]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1259, 0.1126, 0.1193, 0.1286, 0.1279, 0.1255, 0.1284, 0.1318],
        [0.1223, 0.1134, 0.1189, 0.1276, 0.1306, 0.1296, 0.1265, 0.1311],
        [0.1143, 0.1080, 0.1168, 0.1290, 0.1299, 0.1316, 0.1298, 0.1407],
        [0.1131, 0.1074, 0.1127, 0.1347, 0.1294, 0.1323, 0.1318, 0.1386],
        [0.1123, 0.1071, 0.1113, 0.1323, 0.1353, 0.1318, 0.1319, 0.1379]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1333, 0.1246, 0.1247, 0.1267, 0.1277, 0.1229, 0.1193, 0.1209],
        [0.1281, 0.1183, 0.1264, 0.1261, 0.1259, 0.1229, 0.1231, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1346, 0.1252, 0.1244, 0.1256, 0.1250, 0.1228, 0.1227, 0.1196],
        [0.1288, 0.1194, 0.1241, 0.1254, 0.1271, 0.1254, 0.1217, 0.1280],
        [0.1277, 0.1141, 0.1249, 0.1242, 0.1261, 0.1255, 0.1307, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1319, 0.1214, 0.1278, 0.1252, 0.1261, 0.1228, 0.1226, 0.1223],
        [0.1290, 0.1198, 0.1249, 0.1235, 0.1278, 0.1273, 0.1231, 0.1246],
        [0.1254, 0.1108, 0.1215, 0.1241, 0.1324, 0.1240, 0.1337, 0.1281],
        [0.1200, 0.1108, 0.1202, 0.1248, 0.1341, 0.1354, 0.1279, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1238, 0.1205, 0.1296, 0.1271, 0.1231, 0.1212, 0.1224],
        [0.1286, 0.1201, 0.1244, 0.1250, 0.1241, 0.1262, 0.1247, 0.1270],
        [0.1246, 0.1132, 0.1232, 0.1260, 0.1307, 0.1228, 0.1324, 0.1272],
        [0.1195, 0.1117, 0.1215, 0.1313, 0.1306, 0.1285, 0.1291, 0.1278],
        [0.1213, 0.1146, 0.1232, 0.1263, 0.1255, 0.1276, 0.1313, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 11:51:09 PM | Train: [ 3/50] Step 000/781 Loss 1.001 Prec@(1,5) (53.1%, 100.0%)
06/28 11:54:49 PM | Train: [ 3/50] Step 050/781 Loss 0.970 Prec@(1,5) (67.5%, 96.8%)
06/28 11:58:29 PM | Train: [ 3/50] Step 100/781 Loss 0.971 Prec@(1,5) (66.7%, 96.9%)
06/29 12:02:09 AM | Train: [ 3/50] Step 150/781 Loss 0.987 Prec@(1,5) (65.8%, 96.9%)
06/29 12:05:49 AM | Train: [ 3/50] Step 200/781 Loss 0.989 Prec@(1,5) (65.7%, 96.8%)
06/29 12:09:29 AM | Train: [ 3/50] Step 250/781 Loss 0.983 Prec@(1,5) (65.9%, 96.8%)
06/29 12:13:09 AM | Train: [ 3/50] Step 300/781 Loss 0.974 Prec@(1,5) (66.3%, 96.8%)
06/29 12:16:49 AM | Train: [ 3/50] Step 350/781 Loss 0.973 Prec@(1,5) (66.4%, 96.8%)
06/29 12:20:29 AM | Train: [ 3/50] Step 400/781 Loss 0.969 Prec@(1,5) (66.6%, 96.9%)
06/29 12:24:10 AM | Train: [ 3/50] Step 450/781 Loss 0.968 Prec@(1,5) (66.7%, 96.9%)
06/29 12:27:50 AM | Train: [ 3/50] Step 500/781 Loss 0.958 Prec@(1,5) (67.0%, 96.9%)
06/29 12:31:30 AM | Train: [ 3/50] Step 550/781 Loss 0.956 Prec@(1,5) (66.9%, 97.0%)
06/30 07:42:30 AM | 
06/30 07:42:30 AM | Parameters:
06/30 07:42:30 AM | ALPHA_LR=0.0003
06/30 07:42:30 AM | ALPHA_WEIGHT_DECAY=0.001
06/30 07:42:30 AM | BATCH_SIZE=32
06/30 07:42:30 AM | DATA_PATH=./data/
06/30 07:42:30 AM | DATASET=cifar10
06/30 07:42:30 AM | EPOCHS=50
06/30 07:42:30 AM | GPUS=[0]
06/30 07:42:30 AM | INIT_CHANNELS=16
06/30 07:42:30 AM | LAYERS=8
06/30 07:42:30 AM | NAME=cifar10
06/30 07:42:30 AM | PATH=searchs/cifar10
06/30 07:42:30 AM | PLOT_PATH=searchs/cifar10/plots
06/30 07:42:30 AM | PRINT_FREQ=50
06/30 07:42:30 AM | SEED=2
06/30 07:42:30 AM | W_GRAD_CLIP=5.0
06/30 07:42:30 AM | W_LR=0.025
06/30 07:42:30 AM | W_LR_MIN=0.001
06/30 07:42:30 AM | W_MOMENTUM=0.9
06/30 07:42:30 AM | W_WEIGHT_DECAY=0.0003
06/30 07:42:30 AM | WORKERS=4
06/30 07:42:30 AM | 
06/30 07:42:30 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 07:48:44 AM | 
06/30 07:48:44 AM | Parameters:
06/30 07:48:44 AM | ALPHA_LR=0.0003
06/30 07:48:44 AM | ALPHA_WEIGHT_DECAY=0.001
06/30 07:48:44 AM | BATCH_SIZE=32
06/30 07:48:44 AM | DATA_PATH=./data/
06/30 07:48:44 AM | DATASET=cifar10
06/30 07:48:44 AM | EPOCHS=50
06/30 07:48:44 AM | GPUS=[0]
06/30 07:48:44 AM | INIT_CHANNELS=16
06/30 07:48:44 AM | LAYERS=8
06/30 07:48:44 AM | NAME=cifar10
06/30 07:48:44 AM | PATH=searchs/cifar10
06/30 07:48:44 AM | PLOT_PATH=searchs/cifar10/plots
06/30 07:48:44 AM | PRINT_FREQ=50
06/30 07:48:44 AM | SEED=2
06/30 07:48:44 AM | W_GRAD_CLIP=5.0
06/30 07:48:44 AM | W_LR=0.025
06/30 07:48:44 AM | W_LR_MIN=0.001
06/30 07:48:44 AM | W_MOMENTUM=0.9
06/30 07:48:44 AM | W_WEIGHT_DECAY=0.0003
06/30 07:48:44 AM | WORKERS=4
06/30 07:48:44 AM | 
06/30 07:48:44 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 07:59:28 AM | 
06/30 07:59:28 AM | Parameters:
06/30 07:59:28 AM | ALPHA_LR=0.0003
06/30 07:59:28 AM | ALPHA_WEIGHT_DECAY=0.001
06/30 07:59:28 AM | BATCH_SIZE=32
06/30 07:59:28 AM | DATA_PATH=./data/
06/30 07:59:28 AM | DATASET=cifar10
06/30 07:59:28 AM | EPOCHS=50
06/30 07:59:28 AM | GPUS=[0]
06/30 07:59:28 AM | INIT_CHANNELS=16
06/30 07:59:28 AM | LAYERS=8
06/30 07:59:28 AM | NAME=cifar10
06/30 07:59:28 AM | PATH=searchs/cifar10
06/30 07:59:28 AM | PLOT_PATH=searchs/cifar10/plots
06/30 07:59:28 AM | PRINT_FREQ=50
06/30 07:59:28 AM | SEED=2
06/30 07:59:28 AM | W_GRAD_CLIP=5.0
06/30 07:59:28 AM | W_LR=0.025
06/30 07:59:28 AM | W_LR_MIN=0.001
06/30 07:59:28 AM | W_MOMENTUM=0.9
06/30 07:59:28 AM | W_WEIGHT_DECAY=0.0003
06/30 07:59:28 AM | WORKERS=4
06/30 07:59:28 AM | 
06/30 07:59:28 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 04:53:40 PM | 
06/30 04:53:40 PM | Parameters:
06/30 04:53:40 PM | ALPHA_LR=0.0003
06/30 04:53:40 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 04:53:40 PM | BATCH_SIZE=32
06/30 04:53:40 PM | DATA_PATH=./data/
06/30 04:53:40 PM | DATASET=cifar10
06/30 04:53:40 PM | EPOCHS=50
06/30 04:53:40 PM | GPUS=[0]
06/30 04:53:40 PM | INIT_CHANNELS=16
06/30 04:53:40 PM | LAYERS=8
06/30 04:53:40 PM | NAME=cifar10
06/30 04:53:40 PM | PATH=searchs/cifar10
06/30 04:53:40 PM | PLOT_PATH=searchs/cifar10/plots
06/30 04:53:40 PM | PRINT_FREQ=50
06/30 04:53:40 PM | SEED=2
06/30 04:53:40 PM | W_GRAD_CLIP=5.0
06/30 04:53:40 PM | W_LR=0.025
06/30 04:53:40 PM | W_LR_MIN=0.001
06/30 04:53:40 PM | W_MOMENTUM=0.9
06/30 04:53:40 PM | W_WEIGHT_DECAY=0.0003
06/30 04:53:40 PM | WORKERS=4
06/30 04:53:40 PM | 
06/30 04:53:40 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:11:44 PM | 
06/30 05:11:44 PM | Parameters:
06/30 05:11:44 PM | ALPHA_LR=0.0003
06/30 05:11:44 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:11:44 PM | BATCH_SIZE=32
06/30 05:11:44 PM | DATA_PATH=./data/
06/30 05:11:44 PM | DATASET=cifar10
06/30 05:11:44 PM | EPOCHS=50
06/30 05:11:44 PM | GPUS=[0]
06/30 05:11:44 PM | INIT_CHANNELS=16
06/30 05:11:44 PM | LAYERS=8
06/30 05:11:44 PM | NAME=cifar10
06/30 05:11:44 PM | PATH=searchs/cifar10
06/30 05:11:44 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:11:44 PM | PRINT_FREQ=50
06/30 05:11:44 PM | SEED=2
06/30 05:11:44 PM | W_GRAD_CLIP=5.0
06/30 05:11:44 PM | W_LR=0.025
06/30 05:11:44 PM | W_LR_MIN=0.001
06/30 05:11:44 PM | W_MOMENTUM=0.9
06/30 05:11:44 PM | W_WEIGHT_DECAY=0.0003
06/30 05:11:44 PM | WORKERS=4
06/30 05:11:44 PM | 
06/30 05:11:44 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:13:32 PM | 
06/30 05:13:32 PM | Parameters:
06/30 05:13:32 PM | ALPHA_LR=0.0003
06/30 05:13:32 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:13:32 PM | BATCH_SIZE=32
06/30 05:13:32 PM | DATA_PATH=./data/
06/30 05:13:32 PM | DATASET=cifar10
06/30 05:13:32 PM | EPOCHS=50
06/30 05:13:32 PM | GPUS=[0]
06/30 05:13:32 PM | INIT_CHANNELS=16
06/30 05:13:32 PM | LAYERS=8
06/30 05:13:32 PM | NAME=cifar10
06/30 05:13:32 PM | PATH=searchs/cifar10
06/30 05:13:32 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:13:32 PM | PRINT_FREQ=50
06/30 05:13:32 PM | SEED=2
06/30 05:13:32 PM | W_GRAD_CLIP=5.0
06/30 05:13:32 PM | W_LR=0.025
06/30 05:13:32 PM | W_LR_MIN=0.001
06/30 05:13:32 PM | W_MOMENTUM=0.9
06/30 05:13:32 PM | W_WEIGHT_DECAY=0.0003
06/30 05:13:32 PM | WORKERS=4
06/30 05:13:32 PM | 
06/30 05:13:32 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:14:08 PM | 
06/30 05:14:08 PM | Parameters:
06/30 05:14:08 PM | ALPHA_LR=0.0003
06/30 05:14:08 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:14:08 PM | BATCH_SIZE=32
06/30 05:14:08 PM | DATA_PATH=./data/
06/30 05:14:08 PM | DATASET=cifar10
06/30 05:14:08 PM | EPOCHS=50
06/30 05:14:08 PM | GPUS=[0]
06/30 05:14:08 PM | INIT_CHANNELS=16
06/30 05:14:08 PM | LAYERS=8
06/30 05:14:08 PM | NAME=cifar10
06/30 05:14:08 PM | PATH=searchs/cifar10
06/30 05:14:08 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:14:08 PM | PRINT_FREQ=50
06/30 05:14:08 PM | SEED=2
06/30 05:14:08 PM | W_GRAD_CLIP=5.0
06/30 05:14:08 PM | W_LR=0.025
06/30 05:14:08 PM | W_LR_MIN=0.001
06/30 05:14:08 PM | W_MOMENTUM=0.9
06/30 05:14:08 PM | W_WEIGHT_DECAY=0.0003
06/30 05:14:08 PM | WORKERS=4
06/30 05:14:08 PM | 
06/30 05:14:08 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:15:43 PM | 
06/30 05:15:43 PM | Parameters:
06/30 05:15:43 PM | ALPHA_LR=0.0003
06/30 05:15:43 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:15:43 PM | BATCH_SIZE=32
06/30 05:15:43 PM | DATA_PATH=./data/
06/30 05:15:43 PM | DATASET=cifar10
06/30 05:15:43 PM | EPOCHS=50
06/30 05:15:43 PM | GPUS=[0]
06/30 05:15:43 PM | INIT_CHANNELS=16
06/30 05:15:43 PM | LAYERS=8
06/30 05:15:43 PM | NAME=cifar10
06/30 05:15:43 PM | PATH=searchs/cifar10
06/30 05:15:43 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:15:43 PM | PRINT_FREQ=50
06/30 05:15:43 PM | SEED=2
06/30 05:15:43 PM | W_GRAD_CLIP=5.0
06/30 05:15:43 PM | W_LR=0.025
06/30 05:15:43 PM | W_LR_MIN=0.001
06/30 05:15:43 PM | W_MOMENTUM=0.9
06/30 05:15:43 PM | W_WEIGHT_DECAY=0.0003
06/30 05:15:43 PM | WORKERS=4
06/30 05:15:43 PM | 
06/30 05:15:43 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:17:17 PM | 
06/30 05:17:17 PM | Parameters:
06/30 05:17:17 PM | ALPHA_LR=0.0003
06/30 05:17:17 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:17:17 PM | BATCH_SIZE=32
06/30 05:17:17 PM | DATA_PATH=./data/
06/30 05:17:17 PM | DATASET=cifar10
06/30 05:17:17 PM | EPOCHS=50
06/30 05:17:17 PM | GPUS=[0]
06/30 05:17:17 PM | INIT_CHANNELS=16
06/30 05:17:17 PM | LAYERS=8
06/30 05:17:17 PM | NAME=cifar10
06/30 05:17:17 PM | PATH=searchs/cifar10
06/30 05:17:17 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:17:17 PM | PRINT_FREQ=50
06/30 05:17:17 PM | SEED=2
06/30 05:17:17 PM | W_GRAD_CLIP=5.0
06/30 05:17:17 PM | W_LR=0.025
06/30 05:17:17 PM | W_LR_MIN=0.001
06/30 05:17:17 PM | W_MOMENTUM=0.9
06/30 05:17:17 PM | W_WEIGHT_DECAY=0.0003
06/30 05:17:17 PM | WORKERS=4
06/30 05:17:17 PM | 
06/30 05:17:17 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:20:06 PM | 
06/30 05:20:06 PM | Parameters:
06/30 05:20:06 PM | ALPHA_LR=0.0003
06/30 05:20:06 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:20:06 PM | BATCH_SIZE=32
06/30 05:20:06 PM | DATA_PATH=./data/
06/30 05:20:06 PM | DATASET=cifar10
06/30 05:20:06 PM | EPOCHS=50
06/30 05:20:06 PM | GPUS=[0]
06/30 05:20:06 PM | INIT_CHANNELS=16
06/30 05:20:06 PM | LAYERS=8
06/30 05:20:06 PM | NAME=cifar10
06/30 05:20:06 PM | PATH=searchs/cifar10
06/30 05:20:06 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:20:06 PM | PRINT_FREQ=50
06/30 05:20:06 PM | SEED=2
06/30 05:20:06 PM | W_GRAD_CLIP=5.0
06/30 05:20:06 PM | W_LR=0.025
06/30 05:20:06 PM | W_LR_MIN=0.001
06/30 05:20:06 PM | W_MOMENTUM=0.9
06/30 05:20:06 PM | W_WEIGHT_DECAY=0.0003
06/30 05:20:06 PM | WORKERS=4
06/30 05:20:06 PM | 
06/30 05:20:06 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:20:49 PM | 
06/30 05:20:49 PM | Parameters:
06/30 05:20:49 PM | ALPHA_LR=0.0003
06/30 05:20:49 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:20:49 PM | BATCH_SIZE=32
06/30 05:20:49 PM | DATA_PATH=./data/
06/30 05:20:49 PM | DATASET=cifar10
06/30 05:20:49 PM | EPOCHS=50
06/30 05:20:49 PM | GPUS=[0]
06/30 05:20:49 PM | INIT_CHANNELS=16
06/30 05:20:49 PM | LAYERS=8
06/30 05:20:49 PM | NAME=cifar10
06/30 05:20:49 PM | PATH=searchs/cifar10
06/30 05:20:49 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:20:49 PM | PRINT_FREQ=50
06/30 05:20:49 PM | SEED=2
06/30 05:20:49 PM | W_GRAD_CLIP=5.0
06/30 05:20:49 PM | W_LR=0.025
06/30 05:20:49 PM | W_LR_MIN=0.001
06/30 05:20:49 PM | W_MOMENTUM=0.9
06/30 05:20:49 PM | W_WEIGHT_DECAY=0.0003
06/30 05:20:49 PM | WORKERS=4
06/30 05:20:49 PM | 
06/30 05:20:49 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:21:26 PM | 
06/30 05:21:26 PM | Parameters:
06/30 05:21:26 PM | ALPHA_LR=0.0003
06/30 05:21:26 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:21:26 PM | BATCH_SIZE=32
06/30 05:21:26 PM | DATA_PATH=./data/
06/30 05:21:26 PM | DATASET=cifar10
06/30 05:21:26 PM | EPOCHS=50
06/30 05:21:26 PM | GPUS=[0]
06/30 05:21:26 PM | INIT_CHANNELS=16
06/30 05:21:26 PM | LAYERS=8
06/30 05:21:26 PM | NAME=cifar10
06/30 05:21:26 PM | PATH=searchs/cifar10
06/30 05:21:26 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:21:26 PM | PRINT_FREQ=50
06/30 05:21:26 PM | SEED=2
06/30 05:21:26 PM | W_GRAD_CLIP=5.0
06/30 05:21:26 PM | W_LR=0.025
06/30 05:21:26 PM | W_LR_MIN=0.001
06/30 05:21:26 PM | W_MOMENTUM=0.9
06/30 05:21:26 PM | W_WEIGHT_DECAY=0.0003
06/30 05:21:26 PM | WORKERS=4
06/30 05:21:26 PM | 
06/30 05:21:26 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:22:39 PM | 
06/30 05:22:39 PM | Parameters:
06/30 05:22:39 PM | ALPHA_LR=0.0003
06/30 05:22:39 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:22:39 PM | BATCH_SIZE=32
06/30 05:22:39 PM | DATA_PATH=./data/
06/30 05:22:39 PM | DATASET=cifar10
06/30 05:22:39 PM | EPOCHS=50
06/30 05:22:39 PM | GPUS=[0]
06/30 05:22:39 PM | INIT_CHANNELS=16
06/30 05:22:39 PM | LAYERS=8
06/30 05:22:39 PM | NAME=cifar10
06/30 05:22:39 PM | PATH=searchs/cifar10
06/30 05:22:39 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:22:39 PM | PRINT_FREQ=50
06/30 05:22:39 PM | SEED=2
06/30 05:22:39 PM | W_GRAD_CLIP=5.0
06/30 05:22:39 PM | W_LR=0.025
06/30 05:22:39 PM | W_LR_MIN=0.001
06/30 05:22:39 PM | W_MOMENTUM=0.9
06/30 05:22:39 PM | W_WEIGHT_DECAY=0.0003
06/30 05:22:39 PM | WORKERS=4
06/30 05:22:39 PM | 
06/30 05:22:39 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:29:42 PM | 
06/30 05:29:42 PM | Parameters:
06/30 05:29:42 PM | ALPHA_LR=0.0003
06/30 05:29:42 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:29:42 PM | BATCH_SIZE=32
06/30 05:29:42 PM | DATA_PATH=./data/
06/30 05:29:42 PM | DATASET=cifar10
06/30 05:29:42 PM | EPOCHS=50
06/30 05:29:42 PM | GPUS=[0]
06/30 05:29:42 PM | INIT_CHANNELS=16
06/30 05:29:42 PM | LAYERS=8
06/30 05:29:42 PM | NAME=cifar10
06/30 05:29:42 PM | PATH=searchs/cifar10
06/30 05:29:42 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:29:42 PM | PRINT_FREQ=50
06/30 05:29:42 PM | SEED=2
06/30 05:29:42 PM | W_GRAD_CLIP=5.0
06/30 05:29:42 PM | W_LR=0.025
06/30 05:29:42 PM | W_LR_MIN=0.001
06/30 05:29:42 PM | W_MOMENTUM=0.9
06/30 05:29:42 PM | W_WEIGHT_DECAY=0.0003
06/30 05:29:42 PM | WORKERS=4
06/30 05:29:42 PM | 
06/30 05:29:42 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:30:53 PM | 
06/30 05:30:53 PM | Parameters:
06/30 05:30:53 PM | ALPHA_LR=0.0003
06/30 05:30:53 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:30:53 PM | BATCH_SIZE=32
06/30 05:30:53 PM | DATA_PATH=./data/
06/30 05:30:53 PM | DATASET=cifar10
06/30 05:30:53 PM | EPOCHS=50
06/30 05:30:53 PM | GPUS=[0]
06/30 05:30:53 PM | INIT_CHANNELS=16
06/30 05:30:53 PM | LAYERS=8
06/30 05:30:53 PM | NAME=cifar10
06/30 05:30:53 PM | PATH=searchs/cifar10
06/30 05:30:53 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:30:53 PM | PRINT_FREQ=50
06/30 05:30:53 PM | SEED=2
06/30 05:30:53 PM | W_GRAD_CLIP=5.0
06/30 05:30:53 PM | W_LR=0.025
06/30 05:30:53 PM | W_LR_MIN=0.001
06/30 05:30:53 PM | W_MOMENTUM=0.9
06/30 05:30:53 PM | W_WEIGHT_DECAY=0.0003
06/30 05:30:53 PM | WORKERS=4
06/30 05:30:53 PM | 
06/30 05:30:53 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:31:17 PM | 
06/30 05:31:17 PM | Parameters:
06/30 05:31:17 PM | ALPHA_LR=0.0003
06/30 05:31:17 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:31:17 PM | BATCH_SIZE=32
06/30 05:31:17 PM | DATA_PATH=./data/
06/30 05:31:17 PM | DATASET=cifar10
06/30 05:31:17 PM | EPOCHS=50
06/30 05:31:17 PM | GPUS=[0]
06/30 05:31:17 PM | INIT_CHANNELS=16
06/30 05:31:17 PM | LAYERS=8
06/30 05:31:17 PM | NAME=cifar10
06/30 05:31:17 PM | PATH=searchs/cifar10
06/30 05:31:17 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:31:17 PM | PRINT_FREQ=50
06/30 05:31:17 PM | SEED=2
06/30 05:31:17 PM | W_GRAD_CLIP=5.0
06/30 05:31:17 PM | W_LR=0.025
06/30 05:31:17 PM | W_LR_MIN=0.001
06/30 05:31:17 PM | W_MOMENTUM=0.9
06/30 05:31:17 PM | W_WEIGHT_DECAY=0.0003
06/30 05:31:17 PM | WORKERS=4
06/30 05:31:17 PM | 
06/30 05:31:17 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:32:59 PM | 
06/30 05:32:59 PM | Parameters:
06/30 05:32:59 PM | ALPHA_LR=0.0003
06/30 05:32:59 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:32:59 PM | BATCH_SIZE=32
06/30 05:32:59 PM | DATA_PATH=./data/
06/30 05:32:59 PM | DATASET=cifar10
06/30 05:32:59 PM | EPOCHS=50
06/30 05:32:59 PM | GPUS=[0]
06/30 05:32:59 PM | INIT_CHANNELS=16
06/30 05:32:59 PM | LAYERS=8
06/30 05:32:59 PM | NAME=cifar10
06/30 05:32:59 PM | PATH=searchs/cifar10
06/30 05:32:59 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:32:59 PM | PRINT_FREQ=50
06/30 05:32:59 PM | SEED=2
06/30 05:32:59 PM | W_GRAD_CLIP=5.0
06/30 05:32:59 PM | W_LR=0.025
06/30 05:32:59 PM | W_LR_MIN=0.001
06/30 05:32:59 PM | W_MOMENTUM=0.9
06/30 05:32:59 PM | W_WEIGHT_DECAY=0.0003
06/30 05:32:59 PM | WORKERS=4
06/30 05:32:59 PM | 
06/30 05:32:59 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:34:50 PM | 
06/30 05:34:50 PM | Parameters:
06/30 05:34:50 PM | ALPHA_LR=0.0003
06/30 05:34:50 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:34:50 PM | BATCH_SIZE=32
06/30 05:34:50 PM | DATA_PATH=./data/
06/30 05:34:50 PM | DATASET=cifar10
06/30 05:34:50 PM | EPOCHS=50
06/30 05:34:50 PM | GPUS=[0]
06/30 05:34:50 PM | INIT_CHANNELS=16
06/30 05:34:50 PM | LAYERS=8
06/30 05:34:50 PM | NAME=cifar10
06/30 05:34:50 PM | PATH=searchs/cifar10
06/30 05:34:50 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:34:50 PM | PRINT_FREQ=50
06/30 05:34:50 PM | SEED=2
06/30 05:34:50 PM | W_GRAD_CLIP=5.0
06/30 05:34:50 PM | W_LR=0.025
06/30 05:34:50 PM | W_LR_MIN=0.001
06/30 05:34:50 PM | W_MOMENTUM=0.9
06/30 05:34:50 PM | W_WEIGHT_DECAY=0.0003
06/30 05:34:50 PM | WORKERS=4
06/30 05:34:50 PM | 
06/30 05:34:50 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:35:01 PM | Train: [ 1/50] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
06/30 05:35:49 PM | 
06/30 05:35:49 PM | Parameters:
06/30 05:35:49 PM | ALPHA_LR=0.0003
06/30 05:35:49 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:35:49 PM | BATCH_SIZE=32
06/30 05:35:49 PM | DATA_PATH=./data/
06/30 05:35:49 PM | DATASET=cifar10
06/30 05:35:49 PM | EPOCHS=50
06/30 05:35:49 PM | GPUS=[0]
06/30 05:35:49 PM | INIT_CHANNELS=16
06/30 05:35:49 PM | LAYERS=8
06/30 05:35:49 PM | NAME=cifar10
06/30 05:35:49 PM | PATH=searchs/cifar10
06/30 05:35:49 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:35:49 PM | PRINT_FREQ=50
06/30 05:35:49 PM | SEED=2
06/30 05:35:49 PM | W_GRAD_CLIP=5.0
06/30 05:35:49 PM | W_LR=0.025
06/30 05:35:49 PM | W_LR_MIN=0.001
06/30 05:35:49 PM | W_MOMENTUM=0.9
06/30 05:35:49 PM | W_WEIGHT_DECAY=0.0003
06/30 05:35:49 PM | WORKERS=4
06/30 05:35:49 PM | 
06/30 05:35:49 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:36:01 PM | Train: [ 1/50] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
06/30 05:36:57 PM | 
06/30 05:36:57 PM | Parameters:
06/30 05:36:57 PM | ALPHA_LR=0.0003
06/30 05:36:57 PM | ALPHA_WEIGHT_DECAY=0.001
06/30 05:36:57 PM | BATCH_SIZE=32
06/30 05:36:57 PM | DATA_PATH=./data/
06/30 05:36:57 PM | DATASET=cifar10
06/30 05:36:57 PM | EPOCHS=50
06/30 05:36:57 PM | GPUS=[0]
06/30 05:36:57 PM | INIT_CHANNELS=16
06/30 05:36:57 PM | LAYERS=8
06/30 05:36:57 PM | NAME=cifar10
06/30 05:36:57 PM | PATH=searchs/cifar10
06/30 05:36:57 PM | PLOT_PATH=searchs/cifar10/plots
06/30 05:36:57 PM | PRINT_FREQ=50
06/30 05:36:57 PM | SEED=2
06/30 05:36:57 PM | W_GRAD_CLIP=5.0
06/30 05:36:57 PM | W_LR=0.025
06/30 05:36:57 PM | W_LR_MIN=0.001
06/30 05:36:57 PM | W_MOMENTUM=0.9
06/30 05:36:57 PM | W_WEIGHT_DECAY=0.0003
06/30 05:36:57 PM | WORKERS=4
06/30 05:36:57 PM | 
06/30 05:36:57 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 05:37:08 PM | Train: [ 1/50] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
06/30 05:42:41 PM | Train: [ 1/50] Step 050/781 Loss 2.199 Prec@(1,5) (24.0%, 71.7%)
06/30 05:48:13 PM | Train: [ 1/50] Step 100/781 Loss 2.136 Prec@(1,5) (27.6%, 77.0%)
06/30 05:53:45 PM | Train: [ 1/50] Step 150/781 Loss 2.020 Prec@(1,5) (31.4%, 80.5%)
06/30 05:59:16 PM | Train: [ 1/50] Step 200/781 Loss 1.897 Prec@(1,5) (34.9%, 83.4%)
06/30 06:04:47 PM | Train: [ 1/50] Step 250/781 Loss 1.819 Prec@(1,5) (37.6%, 85.0%)
06/30 06:10:19 PM | Train: [ 1/50] Step 300/781 Loss 1.740 Prec@(1,5) (40.3%, 86.5%)
06/30 06:15:51 PM | Train: [ 1/50] Step 350/781 Loss 1.680 Prec@(1,5) (42.3%, 87.6%)
06/30 06:21:21 PM | Train: [ 1/50] Step 400/781 Loss 1.633 Prec@(1,5) (43.9%, 88.2%)
06/30 06:26:52 PM | Train: [ 1/50] Step 450/781 Loss 1.595 Prec@(1,5) (45.2%, 88.9%)
06/30 06:32:24 PM | Train: [ 1/50] Step 500/781 Loss 1.548 Prec@(1,5) (46.8%, 89.7%)
06/30 06:37:55 PM | Train: [ 1/50] Step 550/781 Loss 1.513 Prec@(1,5) (48.0%, 90.2%)
06/30 06:43:27 PM | Train: [ 1/50] Step 600/781 Loss 1.486 Prec@(1,5) (49.0%, 90.5%)
06/30 06:48:58 PM | Train: [ 1/50] Step 650/781 Loss 1.463 Prec@(1,5) (49.8%, 90.9%)
06/30 06:54:31 PM | Train: [ 1/50] Step 700/781 Loss 1.439 Prec@(1,5) (50.7%, 91.2%)
06/30 07:00:00 PM | Train: [ 1/50] Step 750/781 Loss 1.414 Prec@(1,5) (51.6%, 91.6%)
06/30 07:03:26 PM | Train: [ 1/50] Step 781/781 Loss 1.402 Prec@(1,5) (52.1%, 91.7%)
06/30 07:03:26 PM | Train: [ 1/50] Final Prec@1 52.0600%
06/30 07:03:27 PM | Valid: [ 1/50] Step 000/781 Loss 0.949 Prec@(1,5) (68.8%, 96.9%)
06/30 07:04:09 PM | Valid: [ 1/50] Step 050/781 Loss 1.079 Prec@(1,5) (62.3%, 97.2%)
06/30 07:04:51 PM | Valid: [ 1/50] Step 100/781 Loss 1.068 Prec@(1,5) (62.2%, 97.3%)
06/30 07:05:32 PM | Valid: [ 1/50] Step 150/781 Loss 1.072 Prec@(1,5) (63.0%, 96.9%)
06/30 07:06:14 PM | Valid: [ 1/50] Step 200/781 Loss 1.066 Prec@(1,5) (63.2%, 96.9%)
06/30 07:06:55 PM | Valid: [ 1/50] Step 250/781 Loss 1.065 Prec@(1,5) (63.2%, 96.7%)
06/30 07:07:37 PM | Valid: [ 1/50] Step 300/781 Loss 1.071 Prec@(1,5) (63.1%, 96.6%)
06/30 07:08:18 PM | Valid: [ 1/50] Step 350/781 Loss 1.073 Prec@(1,5) (62.9%, 96.6%)
06/30 07:09:00 PM | Valid: [ 1/50] Step 400/781 Loss 1.067 Prec@(1,5) (63.1%, 96.7%)
06/30 07:09:41 PM | Valid: [ 1/50] Step 450/781 Loss 1.067 Prec@(1,5) (63.1%, 96.7%)
06/30 07:10:23 PM | Valid: [ 1/50] Step 500/781 Loss 1.064 Prec@(1,5) (63.1%, 96.7%)
06/30 07:11:05 PM | Valid: [ 1/50] Step 550/781 Loss 1.067 Prec@(1,5) (63.0%, 96.6%)
06/30 07:11:46 PM | Valid: [ 1/50] Step 600/781 Loss 1.064 Prec@(1,5) (63.0%, 96.7%)
06/30 07:12:27 PM | Valid: [ 1/50] Step 650/781 Loss 1.062 Prec@(1,5) (63.1%, 96.6%)
06/30 07:13:09 PM | Valid: [ 1/50] Step 700/781 Loss 1.063 Prec@(1,5) (63.1%, 96.6%)
06/30 07:13:50 PM | Valid: [ 1/50] Step 750/781 Loss 1.061 Prec@(1,5) (63.2%, 96.6%)
06/30 07:14:16 PM | Valid: [ 1/50] Step 781/781 Loss 1.063 Prec@(1,5) (63.2%, 96.7%)
06/30 07:14:16 PM | Valid: [ 1/50] Final Prec@1 63.1600%
06/30 07:14:16 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1273, 0.1277, 0.1273, 0.1234, 0.1225, 0.1202, 0.1238, 0.1279],
        [0.1230, 0.1226, 0.1232, 0.1258, 0.1252, 0.1255, 0.1243, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1257, 0.1256, 0.1255, 0.1248, 0.1223, 0.1252, 0.1239, 0.1270],
        [0.1238, 0.1231, 0.1240, 0.1265, 0.1235, 0.1240, 0.1251, 0.1301],
        [0.1199, 0.1197, 0.1247, 0.1260, 0.1248, 0.1262, 0.1259, 0.1329]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1258, 0.1249, 0.1247, 0.1233, 0.1242, 0.1247, 0.1250, 0.1274],
        [0.1233, 0.1228, 0.1234, 0.1248, 0.1248, 0.1248, 0.1260, 0.1301],
        [0.1194, 0.1193, 0.1239, 0.1263, 0.1263, 0.1265, 0.1257, 0.1325],
        [0.1185, 0.1181, 0.1224, 0.1257, 0.1261, 0.1255, 0.1288, 0.1349]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1234, 0.1234, 0.1230, 0.1256, 0.1262, 0.1262, 0.1243, 0.1278],
        [0.1228, 0.1230, 0.1237, 0.1245, 0.1267, 0.1258, 0.1241, 0.1294],
        [0.1183, 0.1183, 0.1232, 0.1265, 0.1261, 0.1281, 0.1258, 0.1337],
        [0.1159, 0.1157, 0.1201, 0.1293, 0.1276, 0.1281, 0.1288, 0.1346],
        [0.1140, 0.1141, 0.1191, 0.1286, 0.1283, 0.1303, 0.1288, 0.1367]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1282, 0.1281, 0.1247, 0.1233, 0.1245, 0.1237, 0.1240, 0.1235],
        [0.1252, 0.1255, 0.1243, 0.1242, 0.1240, 0.1261, 0.1244, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1285, 0.1278, 0.1264, 0.1235, 0.1244, 0.1239, 0.1236, 0.1219],
        [0.1255, 0.1253, 0.1260, 0.1245, 0.1248, 0.1244, 0.1243, 0.1251],
        [0.1246, 0.1248, 0.1267, 0.1244, 0.1236, 0.1245, 0.1244, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1280, 0.1235, 0.1248, 0.1227, 0.1248, 0.1245, 0.1233],
        [0.1261, 0.1260, 0.1243, 0.1245, 0.1246, 0.1242, 0.1255, 0.1248],
        [0.1249, 0.1253, 0.1274, 0.1242, 0.1252, 0.1230, 0.1237, 0.1262],
        [0.1243, 0.1243, 0.1270, 0.1239, 0.1260, 0.1239, 0.1252, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1283, 0.1245, 0.1237, 0.1241, 0.1236, 0.1251, 0.1222],
        [0.1250, 0.1248, 0.1250, 0.1257, 0.1259, 0.1245, 0.1235, 0.1257],
        [0.1243, 0.1243, 0.1263, 0.1246, 0.1261, 0.1248, 0.1239, 0.1257],
        [0.1241, 0.1239, 0.1264, 0.1251, 0.1244, 0.1255, 0.1248, 0.1258],
        [0.1241, 0.1239, 0.1254, 0.1244, 0.1254, 0.1253, 0.1241, 0.1274]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 07:14:24 PM | Train: [ 2/50] Step 000/781 Loss 1.421 Prec@(1,5) (50.0%, 93.8%)
06/30 07:19:55 PM | Train: [ 2/50] Step 050/781 Loss 1.124 Prec@(1,5) (59.5%, 96.1%)
06/30 07:25:24 PM | Train: [ 2/50] Step 100/781 Loss 1.089 Prec@(1,5) (62.3%, 96.1%)
06/30 07:30:55 PM | Train: [ 2/50] Step 150/781 Loss 1.076 Prec@(1,5) (63.1%, 96.4%)
06/30 07:36:26 PM | Train: [ 2/50] Step 200/781 Loss 1.081 Prec@(1,5) (63.0%, 96.5%)
06/30 07:41:56 PM | Train: [ 2/50] Step 250/781 Loss 1.091 Prec@(1,5) (63.0%, 96.3%)
06/30 07:47:27 PM | Train: [ 2/50] Step 300/781 Loss 1.081 Prec@(1,5) (63.4%, 96.3%)
06/30 07:52:59 PM | Train: [ 2/50] Step 350/781 Loss 1.076 Prec@(1,5) (63.6%, 96.4%)
06/30 07:58:32 PM | Train: [ 2/50] Step 400/781 Loss 1.069 Prec@(1,5) (63.9%, 96.4%)
06/30 08:04:04 PM | Train: [ 2/50] Step 450/781 Loss 1.057 Prec@(1,5) (64.3%, 96.5%)
06/30 08:09:36 PM | Train: [ 2/50] Step 500/781 Loss 1.043 Prec@(1,5) (64.6%, 96.6%)
06/30 08:15:07 PM | Train: [ 2/50] Step 550/781 Loss 1.032 Prec@(1,5) (65.0%, 96.7%)
06/30 08:20:39 PM | Train: [ 2/50] Step 600/781 Loss 1.027 Prec@(1,5) (65.2%, 96.7%)
06/30 08:26:09 PM | Train: [ 2/50] Step 650/781 Loss 1.025 Prec@(1,5) (65.4%, 96.7%)
06/30 08:31:41 PM | Train: [ 2/50] Step 700/781 Loss 1.022 Prec@(1,5) (65.5%, 96.7%)
06/30 08:37:12 PM | Train: [ 2/50] Step 750/781 Loss 1.020 Prec@(1,5) (65.6%, 96.6%)
06/30 08:40:38 PM | Train: [ 2/50] Step 781/781 Loss 1.016 Prec@(1,5) (65.8%, 96.6%)
06/30 08:40:38 PM | Train: [ 2/50] Final Prec@1 65.7840%
06/30 08:40:39 PM | Valid: [ 2/50] Step 000/781 Loss 0.578 Prec@(1,5) (84.4%, 100.0%)
06/30 08:41:21 PM | Valid: [ 2/50] Step 050/781 Loss 0.885 Prec@(1,5) (70.6%, 97.6%)
06/30 08:42:03 PM | Valid: [ 2/50] Step 100/781 Loss 0.914 Prec@(1,5) (68.8%, 97.8%)
06/30 08:42:44 PM | Valid: [ 2/50] Step 150/781 Loss 0.920 Prec@(1,5) (68.8%, 97.7%)
06/30 08:43:26 PM | Valid: [ 2/50] Step 200/781 Loss 0.924 Prec@(1,5) (68.7%, 97.5%)
06/30 08:44:07 PM | Valid: [ 2/50] Step 250/781 Loss 0.935 Prec@(1,5) (68.3%, 97.4%)
06/30 08:44:48 PM | Valid: [ 2/50] Step 300/781 Loss 0.938 Prec@(1,5) (68.1%, 97.3%)
06/30 08:45:29 PM | Valid: [ 2/50] Step 350/781 Loss 0.940 Prec@(1,5) (68.1%, 97.4%)
06/30 08:46:10 PM | Valid: [ 2/50] Step 400/781 Loss 0.942 Prec@(1,5) (68.2%, 97.4%)
06/30 08:46:52 PM | Valid: [ 2/50] Step 450/781 Loss 0.941 Prec@(1,5) (68.2%, 97.4%)
06/30 08:47:33 PM | Valid: [ 2/50] Step 500/781 Loss 0.941 Prec@(1,5) (68.2%, 97.3%)
06/30 08:48:14 PM | Valid: [ 2/50] Step 550/781 Loss 0.943 Prec@(1,5) (68.2%, 97.3%)
06/30 08:48:55 PM | Valid: [ 2/50] Step 600/781 Loss 0.942 Prec@(1,5) (68.3%, 97.3%)
06/30 08:49:37 PM | Valid: [ 2/50] Step 650/781 Loss 0.945 Prec@(1,5) (68.3%, 97.3%)
06/30 08:50:18 PM | Valid: [ 2/50] Step 700/781 Loss 0.943 Prec@(1,5) (68.3%, 97.3%)
06/30 08:50:59 PM | Valid: [ 2/50] Step 750/781 Loss 0.945 Prec@(1,5) (68.3%, 97.3%)
06/30 08:51:24 PM | Valid: [ 2/50] Step 781/781 Loss 0.943 Prec@(1,5) (68.4%, 97.3%)
06/30 08:51:24 PM | Valid: [ 2/50] Final Prec@1 68.4200%
06/30 08:51:24 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 4), ('sep_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('skip_connect', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1279, 0.1269, 0.1279, 0.1229, 0.1224, 0.1188, 0.1219, 0.1313],
        [0.1217, 0.1205, 0.1223, 0.1263, 0.1249, 0.1267, 0.1247, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1262, 0.1244, 0.1249, 0.1260, 0.1233, 0.1232, 0.1238, 0.1282],
        [0.1230, 0.1218, 0.1233, 0.1286, 0.1231, 0.1232, 0.1254, 0.1317],
        [0.1161, 0.1154, 0.1248, 0.1276, 0.1247, 0.1274, 0.1248, 0.1393]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1273, 0.1244, 0.1256, 0.1229, 0.1259, 0.1214, 0.1240, 0.1284],
        [0.1217, 0.1206, 0.1221, 0.1247, 0.1254, 0.1246, 0.1267, 0.1342],
        [0.1157, 0.1150, 0.1233, 0.1275, 0.1269, 0.1274, 0.1281, 0.1360],
        [0.1135, 0.1129, 0.1203, 0.1276, 0.1273, 0.1253, 0.1317, 0.1414]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1246, 0.1224, 0.1225, 0.1253, 0.1263, 0.1267, 0.1234, 0.1288],
        [0.1226, 0.1220, 0.1238, 0.1255, 0.1263, 0.1261, 0.1224, 0.1314],
        [0.1142, 0.1135, 0.1235, 0.1284, 0.1259, 0.1293, 0.1260, 0.1391],
        [0.1097, 0.1091, 0.1167, 0.1299, 0.1313, 0.1312, 0.1308, 0.1412],
        [0.1068, 0.1065, 0.1141, 0.1318, 0.1299, 0.1344, 0.1319, 0.1445]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1329, 0.1322, 0.1234, 0.1233, 0.1219, 0.1218, 0.1211, 0.1233],
        [0.1266, 0.1262, 0.1240, 0.1234, 0.1237, 0.1251, 0.1250, 0.1261]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1327, 0.1313, 0.1279, 0.1220, 0.1215, 0.1223, 0.1218, 0.1206],
        [0.1271, 0.1261, 0.1259, 0.1239, 0.1231, 0.1249, 0.1244, 0.1246],
        [0.1220, 0.1223, 0.1276, 0.1262, 0.1219, 0.1264, 0.1255, 0.1282]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1313, 0.1301, 0.1225, 0.1251, 0.1221, 0.1259, 0.1218, 0.1212],
        [0.1275, 0.1267, 0.1234, 0.1236, 0.1243, 0.1252, 0.1252, 0.1242],
        [0.1220, 0.1226, 0.1283, 0.1237, 0.1278, 0.1228, 0.1239, 0.1289],
        [0.1212, 0.1208, 0.1263, 0.1262, 0.1270, 0.1252, 0.1270, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1325, 0.1317, 0.1242, 0.1223, 0.1229, 0.1223, 0.1223, 0.1217],
        [0.1264, 0.1258, 0.1241, 0.1265, 0.1258, 0.1255, 0.1215, 0.1244],
        [0.1226, 0.1225, 0.1279, 0.1236, 0.1278, 0.1229, 0.1252, 0.1275],
        [0.1224, 0.1218, 0.1273, 0.1251, 0.1246, 0.1275, 0.1240, 0.1272],
        [0.1216, 0.1216, 0.1264, 0.1242, 0.1255, 0.1263, 0.1256, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 08:51:32 PM | Train: [ 3/50] Step 000/781 Loss 0.893 Prec@(1,5) (62.5%, 100.0%)
06/30 08:57:04 PM | Train: [ 3/50] Step 050/781 Loss 0.914 Prec@(1,5) (69.3%, 97.5%)
06/30 09:02:34 PM | Train: [ 3/50] Step 100/781 Loss 0.929 Prec@(1,5) (68.6%, 97.6%)
06/30 09:08:06 PM | Train: [ 3/50] Step 150/781 Loss 0.937 Prec@(1,5) (69.0%, 97.4%)
06/30 09:13:39 PM | Train: [ 3/50] Step 200/781 Loss 0.930 Prec@(1,5) (68.9%, 97.3%)
06/30 09:19:09 PM | Train: [ 3/50] Step 250/781 Loss 0.925 Prec@(1,5) (68.8%, 97.4%)
06/30 09:24:40 PM | Train: [ 3/50] Step 300/781 Loss 0.920 Prec@(1,5) (69.2%, 97.3%)
06/30 09:30:10 PM | Train: [ 3/50] Step 350/781 Loss 0.917 Prec@(1,5) (69.4%, 97.3%)
06/30 09:35:42 PM | Train: [ 3/50] Step 400/781 Loss 0.910 Prec@(1,5) (69.6%, 97.3%)
06/30 09:41:14 PM | Train: [ 3/50] Step 450/781 Loss 0.905 Prec@(1,5) (69.8%, 97.4%)
06/30 09:46:47 PM | Train: [ 3/50] Step 500/781 Loss 0.904 Prec@(1,5) (69.8%, 97.4%)
06/30 09:52:18 PM | Train: [ 3/50] Step 550/781 Loss 0.907 Prec@(1,5) (69.6%, 97.4%)
06/30 09:57:47 PM | Train: [ 3/50] Step 600/781 Loss 0.908 Prec@(1,5) (69.5%, 97.4%)
06/30 10:03:20 PM | Train: [ 3/50] Step 650/781 Loss 0.909 Prec@(1,5) (69.3%, 97.4%)
06/30 10:08:50 PM | Train: [ 3/50] Step 700/781 Loss 0.907 Prec@(1,5) (69.4%, 97.4%)
06/30 10:14:21 PM | Train: [ 3/50] Step 750/781 Loss 0.908 Prec@(1,5) (69.4%, 97.4%)
06/30 10:17:45 PM | Train: [ 3/50] Step 781/781 Loss 0.907 Prec@(1,5) (69.5%, 97.4%)
06/30 10:17:45 PM | Train: [ 3/50] Final Prec@1 69.4880%
06/30 10:17:46 PM | Valid: [ 3/50] Step 000/781 Loss 0.742 Prec@(1,5) (75.0%, 100.0%)
06/30 10:18:28 PM | Valid: [ 3/50] Step 050/781 Loss 1.208 Prec@(1,5) (63.0%, 97.3%)
06/30 10:19:10 PM | Valid: [ 3/50] Step 100/781 Loss 1.243 Prec@(1,5) (63.0%, 97.1%)
06/30 10:19:51 PM | Valid: [ 3/50] Step 150/781 Loss 1.252 Prec@(1,5) (62.7%, 97.1%)
06/30 10:20:33 PM | Valid: [ 3/50] Step 200/781 Loss 1.255 Prec@(1,5) (62.9%, 97.2%)
06/30 10:21:14 PM | Valid: [ 3/50] Step 250/781 Loss 1.259 Prec@(1,5) (63.0%, 97.4%)
06/30 10:21:56 PM | Valid: [ 3/50] Step 300/781 Loss 1.243 Prec@(1,5) (63.5%, 97.3%)
06/30 10:22:37 PM | Valid: [ 3/50] Step 350/781 Loss 1.244 Prec@(1,5) (63.5%, 97.3%)
06/30 10:23:18 PM | Valid: [ 3/50] Step 400/781 Loss 1.240 Prec@(1,5) (63.7%, 97.3%)
06/30 10:23:59 PM | Valid: [ 3/50] Step 450/781 Loss 1.235 Prec@(1,5) (63.7%, 97.4%)
06/30 10:24:41 PM | Valid: [ 3/50] Step 500/781 Loss 1.234 Prec@(1,5) (63.8%, 97.3%)
06/30 10:25:22 PM | Valid: [ 3/50] Step 550/781 Loss 1.229 Prec@(1,5) (63.8%, 97.4%)
06/30 10:26:04 PM | Valid: [ 3/50] Step 600/781 Loss 1.229 Prec@(1,5) (63.8%, 97.4%)
06/30 10:26:46 PM | Valid: [ 3/50] Step 650/781 Loss 1.226 Prec@(1,5) (63.8%, 97.5%)
06/30 10:27:27 PM | Valid: [ 3/50] Step 700/781 Loss 1.229 Prec@(1,5) (63.6%, 97.5%)
06/30 10:28:08 PM | Valid: [ 3/50] Step 750/781 Loss 1.230 Prec@(1,5) (63.6%, 97.5%)
06/30 10:28:34 PM | Valid: [ 3/50] Step 781/781 Loss 1.231 Prec@(1,5) (63.5%, 97.4%)
06/30 10:28:34 PM | Valid: [ 3/50] Final Prec@1 63.5480%
06/30 10:28:34 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 3), ('sep_conv_3x3', 2)], [('dil_conv_3x3', 4), ('sep_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1268, 0.1243, 0.1266, 0.1241, 0.1248, 0.1186, 0.1198, 0.1349],
        [0.1214, 0.1195, 0.1222, 0.1272, 0.1253, 0.1276, 0.1222, 0.1345]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1218, 0.1230, 0.1273, 0.1244, 0.1240, 0.1248, 0.1298],
        [0.1224, 0.1208, 0.1226, 0.1291, 0.1235, 0.1217, 0.1266, 0.1332],
        [0.1138, 0.1123, 0.1243, 0.1287, 0.1265, 0.1283, 0.1223, 0.1439]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1263, 0.1218, 0.1240, 0.1236, 0.1273, 0.1211, 0.1254, 0.1305],
        [0.1209, 0.1196, 0.1217, 0.1266, 0.1274, 0.1230, 0.1255, 0.1354],
        [0.1135, 0.1121, 0.1228, 0.1286, 0.1273, 0.1284, 0.1283, 0.1390],
        [0.1103, 0.1094, 0.1184, 0.1284, 0.1281, 0.1276, 0.1327, 0.1451]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1208, 0.1221, 0.1267, 0.1255, 0.1272, 0.1234, 0.1294],
        [0.1221, 0.1209, 0.1235, 0.1264, 0.1289, 0.1259, 0.1218, 0.1306],
        [0.1121, 0.1103, 0.1247, 0.1294, 0.1265, 0.1304, 0.1249, 0.1417],
        [0.1057, 0.1050, 0.1149, 0.1304, 0.1332, 0.1329, 0.1314, 0.1465],
        [0.1022, 0.1019, 0.1118, 0.1331, 0.1315, 0.1353, 0.1331, 0.1511]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1388, 0.1365, 0.1209, 0.1221, 0.1179, 0.1211, 0.1201, 0.1226],
        [0.1275, 0.1272, 0.1229, 0.1219, 0.1239, 0.1244, 0.1257, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1380, 0.1348, 0.1261, 0.1205, 0.1198, 0.1212, 0.1201, 0.1195],
        [0.1284, 0.1276, 0.1246, 0.1251, 0.1218, 0.1236, 0.1251, 0.1238],
        [0.1210, 0.1202, 0.1295, 0.1275, 0.1204, 0.1260, 0.1258, 0.1297]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1347, 0.1322, 0.1217, 0.1262, 0.1200, 0.1257, 0.1189, 0.1206],
        [0.1282, 0.1273, 0.1227, 0.1240, 0.1220, 0.1252, 0.1265, 0.1240],
        [0.1200, 0.1198, 0.1285, 0.1231, 0.1296, 0.1239, 0.1245, 0.1306],
        [0.1189, 0.1180, 0.1264, 0.1281, 0.1292, 0.1228, 0.1293, 0.1273]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1371, 0.1352, 0.1254, 0.1205, 0.1210, 0.1205, 0.1198, 0.1205],
        [0.1264, 0.1261, 0.1225, 0.1268, 0.1260, 0.1265, 0.1214, 0.1243],
        [0.1216, 0.1208, 0.1285, 0.1226, 0.1303, 0.1230, 0.1249, 0.1284],
        [0.1203, 0.1195, 0.1283, 0.1247, 0.1246, 0.1275, 0.1257, 0.1294],
        [0.1198, 0.1193, 0.1261, 0.1257, 0.1245, 0.1281, 0.1268, 0.1297]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/30 10:28:42 PM | Train: [ 4/50] Step 000/781 Loss 1.201 Prec@(1,5) (71.9%, 96.9%)
06/30 10:34:14 PM | Train: [ 4/50] Step 050/781 Loss 0.856 Prec@(1,5) (72.6%, 97.8%)
06/30 10:39:44 PM | Train: [ 4/50] Step 100/781 Loss 0.828 Prec@(1,5) (72.7%, 97.8%)
06/30 10:45:15 PM | Train: [ 4/50] Step 150/781 Loss 0.836 Prec@(1,5) (72.4%, 97.8%)
06/30 10:50:46 PM | Train: [ 4/50] Step 200/781 Loss 0.831 Prec@(1,5) (72.4%, 97.9%)
06/30 10:56:18 PM | Train: [ 4/50] Step 250/781 Loss 0.836 Prec@(1,5) (72.0%, 97.9%)
06/30 11:01:49 PM | Train: [ 4/50] Step 300/781 Loss 0.830 Prec@(1,5) (72.2%, 97.9%)
06/30 11:07:21 PM | Train: [ 4/50] Step 350/781 Loss 0.833 Prec@(1,5) (72.1%, 97.9%)
06/30 11:12:51 PM | Train: [ 4/50] Step 400/781 Loss 0.832 Prec@(1,5) (72.2%, 97.8%)
06/30 11:18:23 PM | Train: [ 4/50] Step 450/781 Loss 0.833 Prec@(1,5) (72.2%, 97.8%)
06/30 11:23:53 PM | Train: [ 4/50] Step 500/781 Loss 0.838 Prec@(1,5) (72.0%, 97.7%)
06/30 11:29:25 PM | Train: [ 4/50] Step 550/781 Loss 0.841 Prec@(1,5) (71.9%, 97.6%)
06/30 11:34:57 PM | Train: [ 4/50] Step 600/781 Loss 0.840 Prec@(1,5) (72.0%, 97.7%)
06/30 11:40:29 PM | Train: [ 4/50] Step 650/781 Loss 0.836 Prec@(1,5) (72.1%, 97.7%)
06/30 11:46:01 PM | Train: [ 4/50] Step 700/781 Loss 0.837 Prec@(1,5) (72.0%, 97.7%)
06/30 11:51:35 PM | Train: [ 4/50] Step 750/781 Loss 0.833 Prec@(1,5) (72.1%, 97.8%)
06/30 11:55:00 PM | Train: [ 4/50] Step 781/781 Loss 0.833 Prec@(1,5) (72.1%, 97.8%)
06/30 11:55:00 PM | Train: [ 4/50] Final Prec@1 72.0840%
06/30 11:55:01 PM | Valid: [ 4/50] Step 000/781 Loss 0.838 Prec@(1,5) (71.9%, 100.0%)
06/30 11:55:43 PM | Valid: [ 4/50] Step 050/781 Loss 0.820 Prec@(1,5) (72.1%, 98.2%)
06/30 11:56:25 PM | Valid: [ 4/50] Step 100/781 Loss 0.802 Prec@(1,5) (72.6%, 98.1%)
06/30 11:57:06 PM | Valid: [ 4/50] Step 150/781 Loss 0.812 Prec@(1,5) (72.6%, 98.0%)
06/30 11:57:48 PM | Valid: [ 4/50] Step 200/781 Loss 0.812 Prec@(1,5) (72.8%, 98.1%)
06/30 11:58:30 PM | Valid: [ 4/50] Step 250/781 Loss 0.814 Prec@(1,5) (72.9%, 98.0%)
06/30 11:59:12 PM | Valid: [ 4/50] Step 300/781 Loss 0.808 Prec@(1,5) (73.1%, 98.1%)
06/30 11:59:53 PM | Valid: [ 4/50] Step 350/781 Loss 0.809 Prec@(1,5) (73.2%, 98.0%)
07/01 12:00:34 AM | Valid: [ 4/50] Step 400/781 Loss 0.811 Prec@(1,5) (73.1%, 98.0%)
07/01 12:01:16 AM | Valid: [ 4/50] Step 450/781 Loss 0.810 Prec@(1,5) (73.1%, 98.1%)
07/01 12:01:57 AM | Valid: [ 4/50] Step 500/781 Loss 0.809 Prec@(1,5) (73.2%, 98.1%)
07/01 12:02:39 AM | Valid: [ 4/50] Step 550/781 Loss 0.809 Prec@(1,5) (73.2%, 98.0%)
07/01 12:03:20 AM | Valid: [ 4/50] Step 600/781 Loss 0.809 Prec@(1,5) (73.2%, 98.0%)
07/01 12:04:02 AM | Valid: [ 4/50] Step 650/781 Loss 0.805 Prec@(1,5) (73.2%, 98.0%)
07/01 12:04:44 AM | Valid: [ 4/50] Step 700/781 Loss 0.806 Prec@(1,5) (73.1%, 98.0%)
07/01 12:05:25 AM | Valid: [ 4/50] Step 750/781 Loss 0.807 Prec@(1,5) (73.1%, 98.1%)
07/01 12:05:51 AM | Valid: [ 4/50] Step 781/781 Loss 0.810 Prec@(1,5) (73.1%, 98.0%)
07/01 12:05:51 AM | Valid: [ 4/50] Final Prec@1 73.0720%
07/01 12:05:51 AM | genotype = Genotype(normal=[[('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 3), ('sep_conv_3x3', 2)], [('dil_conv_3x3', 4), ('dil_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1255, 0.1220, 0.1254, 0.1252, 0.1250, 0.1193, 0.1183, 0.1394],
        [0.1211, 0.1185, 0.1223, 0.1273, 0.1255, 0.1290, 0.1210, 0.1352]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1237, 0.1200, 0.1218, 0.1276, 0.1252, 0.1251, 0.1245, 0.1321],
        [0.1227, 0.1207, 0.1233, 0.1296, 0.1226, 0.1204, 0.1266, 0.1341],
        [0.1115, 0.1096, 0.1243, 0.1287, 0.1270, 0.1280, 0.1220, 0.1490]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1196, 0.1226, 0.1234, 0.1282, 0.1220, 0.1263, 0.1327],
        [0.1209, 0.1191, 0.1221, 0.1265, 0.1286, 0.1224, 0.1241, 0.1364],
        [0.1111, 0.1093, 0.1230, 0.1302, 0.1263, 0.1282, 0.1272, 0.1447],
        [0.1063, 0.1053, 0.1169, 0.1284, 0.1293, 0.1302, 0.1328, 0.1508]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1245, 0.1183, 0.1211, 0.1297, 0.1252, 0.1266, 0.1235, 0.1313],
        [0.1219, 0.1199, 0.1239, 0.1268, 0.1318, 0.1239, 0.1214, 0.1305],
        [0.1094, 0.1070, 0.1249, 0.1303, 0.1283, 0.1312, 0.1237, 0.1453],
        [0.1013, 0.1002, 0.1125, 0.1309, 0.1342, 0.1344, 0.1339, 0.1526],
        [0.0968, 0.0964, 0.1074, 0.1361, 0.1351, 0.1374, 0.1341, 0.1568]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1438, 0.1405, 0.1181, 0.1235, 0.1162, 0.1185, 0.1160, 0.1234],
        [0.1288, 0.1276, 0.1219, 0.1227, 0.1213, 0.1238, 0.1284, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1416, 0.1377, 0.1239, 0.1197, 0.1198, 0.1194, 0.1191, 0.1187],
        [0.1305, 0.1289, 0.1227, 0.1240, 0.1210, 0.1231, 0.1261, 0.1236],
        [0.1201, 0.1184, 0.1307, 0.1282, 0.1211, 0.1266, 0.1259, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1378, 0.1344, 0.1195, 0.1241, 0.1190, 0.1256, 0.1188, 0.1209],
        [0.1294, 0.1276, 0.1218, 0.1233, 0.1204, 0.1255, 0.1289, 0.1230],
        [0.1187, 0.1174, 0.1289, 0.1242, 0.1322, 0.1235, 0.1241, 0.1309],
        [0.1165, 0.1153, 0.1269, 0.1306, 0.1315, 0.1213, 0.1304, 0.1275]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1400, 0.1372, 0.1257, 0.1178, 0.1190, 0.1195, 0.1191, 0.1217],
        [0.1275, 0.1261, 0.1201, 0.1270, 0.1267, 0.1278, 0.1205, 0.1242],
        [0.1202, 0.1184, 0.1287, 0.1234, 0.1334, 0.1230, 0.1247, 0.1283],
        [0.1174, 0.1159, 0.1279, 0.1281, 0.1254, 0.1286, 0.1273, 0.1294],
        [0.1176, 0.1162, 0.1254, 0.1267, 0.1267, 0.1294, 0.1283, 0.1297]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 12:05:59 AM | Train: [ 5/50] Step 000/781 Loss 0.801 Prec@(1,5) (81.2%, 96.9%)
07/01 12:11:31 AM | Train: [ 5/50] Step 050/781 Loss 0.816 Prec@(1,5) (71.9%, 98.2%)
07/01 12:17:03 AM | Train: [ 5/50] Step 100/781 Loss 0.814 Prec@(1,5) (72.6%, 98.5%)
07/01 12:22:34 AM | Train: [ 5/50] Step 150/781 Loss 0.824 Prec@(1,5) (72.2%, 98.3%)
07/01 12:28:06 AM | Train: [ 5/50] Step 200/781 Loss 0.824 Prec@(1,5) (72.2%, 98.2%)
07/01 12:33:37 AM | Train: [ 5/50] Step 250/781 Loss 0.818 Prec@(1,5) (72.3%, 98.2%)
07/01 12:39:09 AM | Train: [ 5/50] Step 300/781 Loss 0.819 Prec@(1,5) (72.1%, 98.2%)
07/01 12:44:40 AM | Train: [ 5/50] Step 350/781 Loss 0.812 Prec@(1,5) (72.5%, 98.1%)
07/01 12:50:13 AM | Train: [ 5/50] Step 400/781 Loss 0.815 Prec@(1,5) (72.5%, 98.0%)
07/01 12:55:43 AM | Train: [ 5/50] Step 450/781 Loss 0.811 Prec@(1,5) (72.5%, 98.1%)
07/01 01:01:16 AM | Train: [ 5/50] Step 500/781 Loss 0.814 Prec@(1,5) (72.4%, 98.1%)
