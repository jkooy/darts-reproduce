{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30 07:59:28 AM | \n",
      "06/30 07:59:28 AM | Parameters:\n",
      "06/30 07:59:28 AM | ALPHA_LR=0.0003\n",
      "06/30 07:59:28 AM | ALPHA_WEIGHT_DECAY=0.001\n",
      "06/30 07:59:28 AM | BATCH_SIZE=32\n",
      "06/30 07:59:28 AM | DATA_PATH=./data/\n",
      "06/30 07:59:28 AM | DATASET=cifar10\n",
      "06/30 07:59:28 AM | EPOCHS=50\n",
      "06/30 07:59:28 AM | GPUS=[0]\n",
      "06/30 07:59:28 AM | INIT_CHANNELS=16\n",
      "06/30 07:59:28 AM | LAYERS=8\n",
      "06/30 07:59:28 AM | NAME=cifar10\n",
      "06/30 07:59:28 AM | PATH=searchs/cifar10\n",
      "06/30 07:59:28 AM | PLOT_PATH=searchs/cifar10/plots\n",
      "06/30 07:59:28 AM | PRINT_FREQ=50\n",
      "06/30 07:59:28 AM | SEED=2\n",
      "06/30 07:59:28 AM | W_GRAD_CLIP=5.0\n",
      "06/30 07:59:28 AM | W_LR=0.025\n",
      "06/30 07:59:28 AM | W_LR_MIN=0.001\n",
      "06/30 07:59:28 AM | W_MOMENTUM=0.9\n",
      "06/30 07:59:28 AM | W_WEIGHT_DECAY=0.0003\n",
      "06/30 07:59:28 AM | WORKERS=4\n",
      "06/30 07:59:28 AM | \n",
      "06/30 07:59:28 AM | Logger is set - training start\n",
      "Files already downloaded and verified\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "####### ALPHA #######\n",
      "# Alpha - normal\n",
      "tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],\n",
      "        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],\n",
      "        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],\n",
      "        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],\n",
      "        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],\n",
      "        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],\n",
      "        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],\n",
      "        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],\n",
      "        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],\n",
      "        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],\n",
      "        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "# Alpha - reduce\n",
      "tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],\n",
      "        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],\n",
      "        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],\n",
      "        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],\n",
      "        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],\n",
      "        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],\n",
      "        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],\n",
      "        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],\n",
      "        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "#####################\n",
      "torch.Size([32, 3, 32, 32])\n",
      "............resnet50 backbone.............\n",
      "Traceback (most recent call last):\n",
      "  File \"search.py\", line 204, in <module>\n",
      "    main()\n",
      "  File \"search.py\", line 87, in main\n",
      "    train(train_loader, valid_loader, model, architect, w_optim, alpha_optim, lr, epoch)\n",
      "  File \"search.py\", line 135, in train\n",
      "    architect.unrolled_backward(trn_X, trn_y, val_X, val_y, lr, w_optim)\n",
      "  File \"/home/ec2-user/pt.darts/architect.py\", line 59, in unrolled_backward\n",
      "    self.virtual_step(trn_X, trn_y, xi, w_optim)\n",
      "  File \"/home/ec2-user/pt.darts/architect.py\", line 34, in virtual_step\n",
      "    loss = self.net.loss(trn_X, trn_y) # L_trn(w)\n",
      "  File \"/home/ec2-user/pt.darts/models/search_cnn.py\", line 135, in loss\n",
      "    logits = self.forward(X)\n",
      "  File \"/home/ec2-user/pt.darts/models/search_cnn.py\", line 119, in forward\n",
      "    return self.net(x, weights_normal, weights_reduce)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ec2-user/pt.darts/models/search_cnn.py\", line 77, in forward\n",
      "    s0, s1 = s1, cell(s0, s1, weights)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ec2-user/pt.darts/models/search_cells.py\", line 49, in forward\n",
      "    s_cur = sum(edges[i](s, w) for i, (s, w) in enumerate(zip(states, w_list)))\n",
      "  File \"/home/ec2-user/pt.darts/models/search_cells.py\", line 49, in <genexpr>\n",
      "    s_cur = sum(edges[i](s, w) for i, (s, w) in enumerate(zip(states, w_list)))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ec2-user/pt.darts/models/ops.py\", line 198, in forward\n",
      "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
      "  File \"/home/ec2-user/pt.darts/models/ops.py\", line 198, in <genexpr>\n",
      "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ec2-user/pt.darts/models/ops.py\", line 178, in forward\n",
      "    out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 345, in forward\n",
      "    return self.conv2d_forward(input, self.weight)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: Calculated padded input size per channel: (0 x 0). Kernel size: (1 x 1). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "!python search.py --name cifar10 --dataset cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
